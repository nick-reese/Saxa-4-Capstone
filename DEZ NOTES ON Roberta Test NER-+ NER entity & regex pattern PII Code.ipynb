{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea11a962",
   "metadata": {},
   "source": [
    "# NICK'S ORIGINAL CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7a1e7",
   "metadata": {},
   "source": [
    "# OBJECTIVE: (TEAM MEETING NOTES)\n",
    "\n",
    "1. Complete EDA analysis on the base NER model. \n",
    "2. After we can begin experimenting with LLM models. \n",
    "3. Once we determine best model, we can move forward with explainability features that work best for our business use case. \n",
    "\n",
    "**Business use case: \n",
    "- individuals trying to explore the job market without industries paying for a first step forward. \n",
    "- A job searching tool that is completely unbiased from other companies, just looking forward with the individual’s best intention to aid in their job hunt. \n",
    "\n",
    "\n",
    "**FOR LATER:\n",
    "\n",
    "Idea: \n",
    "- use API from Bureau of Labor Statistics to use as baseline of occupations and industries (free open source data to extract from) \n",
    "- Job recommendations idea: use cluster analysis to give consumer “jobs you may enjoy if you want to move industries(industries as series, job title as title and if we want to reference NAICS directly, we can reference the 6 digit NAICS code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39244000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "#from docx import Document\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pdfplumber\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import os\n",
    "import json\n",
    "#from docx import Document\n",
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from plotly.offline import plot\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e044a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a22515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b32b0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54816c",
   "metadata": {},
   "source": [
    "## Loading the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = \"\"\"\n",
    "\n",
    "Nicholas E. Reese\n",
    "Van Ness, Washington D.C.\n",
    "412-216-2398\n",
    "Nicholas.E.Reese15@gmail.com \n",
    "Education \n",
    "•Georgetown University - August 2023 – December 2024\n",
    "Master of Science Business Analytics – MSBA candidate\n",
    "Peer-Elected Class Representative \n",
    "•Dickinson College - August 2014 – September 2018\n",
    "Economics & Political Science Double Major\n",
    "Varsity Tennis Captain \n",
    "•University of Bologna - August 2016\n",
    "\n",
    "Skills\n",
    "- R studio\n",
    "- Python\n",
    "- Power BI\n",
    "- SQL\n",
    "- Neural Networks\n",
    "- Machine Learning\n",
    "- Macro Modeling\n",
    "- AWS Cloud Services\n",
    "- Pandas\n",
    "- A/B Testing\n",
    "- Scikit-Learn\n",
    "- Econometrics\n",
    "\n",
    "Professional Experience\n",
    "FINRA, Washington, D.C.\tJune 2020 – Present \n",
    "Senior Analyst, Market Regulation\tSeptember 2022 – Present \n",
    "•Earned top 10% of performance of analysts for past two years.\n",
    "•Led team in research implementing PostgreSQL and NoSQL queries for large data pulls.\n",
    "•Spearheaded new analytical approaches to financial workflow with tools such as R & Python for model development.\n",
    "•Developed working predictive modeling schemas for senior staff using statistical analysis.\n",
    "•Leveraged skills in platforms like Python, R, Power BI, Tableau and SQL accompanied with strong statistical background in financial markets.\n",
    "•Built the Security-Based Swap training manual deck & produced the recorded info session for all of FINRA. \n",
    "•Selected for the FINRA’s first Georgetown Advanced Analytics Program as one of the most junior staff awarded opportunity.\n",
    "•Incorporated statistical analytics to assist in creating a NPL tool to analyze financial documents language to minimize time on manual analysis.\n",
    "•Produced unsupervised and supervised models to perform analysis for Security-Based Swaps trade patterns.\n",
    "•Improved FINRA platforms with Data Scientists for higher accuracy and efficiency. \n",
    "•Managed process of re-engineering supervisory reviews through advanced analytic tools for senior staff.\n",
    "•Implements advanced analytics to assist senior staff with maximizing efficiencies in daily workloads and trade pattern creations.\n",
    "•Presented visualization case work using Power BI & Tableau to senior leadership.\n",
    "•Persuaded senior staff to allow for statistical analytics tools like R.\n",
    "•Mentored over 20 junior & senior staff members on improving processes with analytical tools for financial reviews.\n",
    "•Managed junior staff with day-to-day workload while teaching the staff how to use advanced analytics.\n",
    "Analyst, Market Regulation\tJune 2021-September 2022\n",
    "·Created macroeconomic models for newly implemented FINRA Rule 2232 and MSRB Rule G-15.\n",
    "·Mentored junior staff with creating macro models, synthesizing responses from FINRA member firms.\n",
    "·Won the Regulator Scholarship for continued financial learning based on individual performance.\n",
    "·Received six internal awards for expertise in Municipal and Corporate Bond analysis as an analyst.\n",
    "Associate Analyst, Market Regulation\tJune 2020 – June 2021\n",
    "·Created macroeconomic models to quantify business models of selected firms for FINRA Rule 2232.\n",
    "             BNY Mellon, Pittsburgh, PA\t\t\t\t\t\t                 September 2019 – June 2020\n",
    "Corporate Trust Associate\n",
    "·Leader & instructor of the Bloomberg software for the Corporate Trust Team.\n",
    "LendingHome, Internship, Pittsburgh, PA\n",
    "Funding Specialist & Post Closing Member\tMarch 2019 – August 2019\n",
    "·Reviewed closing documents to ensure precise execution for funding staff.\n",
    "              Veraction/Trax, Junior Business Analyst, Memphis, TN\t  June 2016 - August 2016                                                    June 2016 - August 2016\n",
    "·Led the creation, development, and implementation of Request For Proposal (RFP) database.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16774b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf437695",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d14c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline('ner', model = \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "                       aggregation_strategy = 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ner_pipeline(resume_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c422337",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_entities = [entity for entity in entities if entity['entity_group'] != 'MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011758b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in filtered_entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21cd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern for email extraction\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "\n",
    "# Find all email addresses in the text\n",
    "emails = re.findall(email_pattern, resume_text)\n",
    "\n",
    "# Display found email addresses\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98760ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the NER pipeline\n",
    "ner_pipeline2 = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "\n",
    "# Detect entities using the NER pipeline\n",
    "entities2 = ner_pipeline2(resume_text)\n",
    "\n",
    "# Regex pattern for email extraction\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "\n",
    "# Find all email addresses in the text\n",
    "emails = re.findall(email_pattern, resume_text)\n",
    "\n",
    "# Print detected entities from NER\n",
    "for entity in entities2:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70122958",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c35996",
   "metadata": {},
   "source": [
    "# Dezmond Initial Reactions\n",
    "\n",
    "So far it looks like a single resume has been used for extracting entities and identifying emails.  The model that was used was NER Bert Model.  I think this was done to ensure each step worked before moving forward.\n",
    "\n",
    "The process seems straight forward. But I'd like more resumes for exposure to different resume format, styles, entities and pii values.\n",
    "\n",
    "For the sake of this exercise, I will do the following:\n",
    "- add more regex patterns to help extract pii not captured in entities\n",
    "- use other ner models to detect pii with entities\n",
    "- MAYBE compare ner model results\n",
    "\n",
    "I will make sure not to use a code editing tool to clean up my code or comments as to keep my points and intentions straightforward\n",
    "\n",
    "## What Pii Values do resumes have that need to be identified\n",
    "- phone number\n",
    "- website link (linkedin, social media, git links)\n",
    "- address\n",
    "- name\n",
    "- unique identifiers (tax id #, social security #, etc...)\n",
    "- international identifiers should be considered (Mentioned in Deloitte Meeting)\n",
    "\n",
    "\n",
    "## If Pii is a concern, how do we get rid or at least hide it\n",
    "- redact\n",
    "- replace\n",
    "\n",
    "\n",
    "## What other EDA can I do?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Other Notes,\n",
    "\n",
    "I recoemmedn keeping a running list of all sites where you pull code from directly or reference.  I will add linked to sites i directly reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7d8e0",
   "metadata": {},
   "source": [
    "## regex patterns and ner mode api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other regex patterns for pii values in resumes\n",
    "\n",
    "#us_full_name_pattern = r'\\b[A-Z][a-z]*\\s[A-Z][a-z]*\\b',  # pattern for simpler full name (e.g., John Doe)\n",
    "#international_full_name =\n",
    "\n",
    "us_phone_number = r'(\\+?1\\s?)?(\\(?\\d{3}\\)?[\\s-]?)?\\d{3}[\\s-]?\\d{4}',  # US phone # pattern\n",
    "international_phone = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{1,4}?\\)?[-.\\s]?)?(\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9})'\n",
    "# us_address = r'\\d{1,4}\\s\\w+\\s\\w+\\.?',  # street address ('123 Main St) '' - Legacy\n",
    "us_address = r'([0-9]+)\\s([A-Za-z0-9\\-\\s]+),\\s([A-Za-z\\s]+),\\s([A-Z]{2})\\s([0-9\\-]{5,10})'\n",
    "international_address = r'^\\d{1,5}\\s[\\w\\s.,\\'-]+,\\s*[\\w\\s.,\\'-]+,\\s*[A-Z]{2,3}\\s*\\d{2,5}(-\\d{4})?$'\n",
    "social_security_number = r'\\b\\d{3}-\\d{2}-\\d{4}\\b',  # Social security number pattern (XXX-XX-XXXX)\n",
    "linkedin = r'\\b[a-zA-Z0-9._-]+/in/[a-zA-Z0-9._-]+\\b',  # LinkedIn profile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other ner models + spacy for detecting pii with entities\n",
    "\n",
    "ner_pipeline2 = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\") #BERT\n",
    "ner_pipeline2 = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\") #RoBERTa\n",
    "ner_pipeline2 = pipeline(\"ner\", model=\"albert/albert-base-v1\", aggregation_strategy=\"simple\") # ALBERT\n",
    "ner_pipeline2 = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\") #distilBERT\n",
    "ner_pipeline2 = spacy.load(\"en_core_web_sm\") # Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac0266",
   "metadata": {},
   "source": [
    "## ner model & regex pii identfying code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at how well each ner model or regex pattern identified pii values\n",
    "# Maybe figureout if we can set a threshold score for classifying entities .75 and up maybe...\n",
    "\n",
    "# added regext code:\n",
    "\n",
    "\n",
    "# NER pipeline prewritten for each model in question\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\") #BERT\n",
    "#ner_pipeline = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\") #RoBERTa\n",
    "#ner_pipeline = pipeline(\"ner\", model=\"albert/albert-base-v1\", aggregation_strategy=\"simple\") # ALBERT\n",
    "#ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\") #distilBERT\n",
    "#ner_pipeline = spacy.load(\"en_core_web_sm\") # Spacy\n",
    "\n",
    "# Finding entities using the NER pipeline\n",
    "entities = ner_pipeline2(resume_text)\n",
    "\n",
    "# Regex pattern for email, full name, phone number, address, ssn, linkedin extraction\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "#us_phone_number_pattern = r'(\\+?1\\s?)?(\\(?\\d{3}\\)?[\\s-]?)?\\d{3}[\\s-]?\\d{4}'  # US phone number pattern\n",
    "#international_phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{1,4}?\\)?[-.\\s]?)?(\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9})'  # International phone number pattern\n",
    "#us_address_pattern = r'([0-9]+)\\s([A-Za-z0-9\\-\\s]+),\\s([A-Za-z\\s]+),\\s([A-Z]{2})\\s([0-9\\-]{5,10})'  # US address pattern\n",
    "#international_address_pattern = r'^\\d{1,5}\\s[\\w\\s.,\\'-]+,\\s*[\\w\\s.,\\'-]+,\\s*[A-Z]{2,3}\\s*\\d{2,5}(-\\d{4})?$'  # International address pattern\n",
    "#social_security_number_pattern = r'\\b\\d{3}-\\d{2}-\\d{4}\\b'  # Social Security Number pattern (XXX-XX-XXXX)\n",
    "#linkedin_pattern = r'\\b(?:https?://)?(?:www\\.)?linkedin\\.com/in/[a-zA-Z0-9._-]+\\b'  # LinkedIn profile pattern\n",
    "\n",
    "\n",
    "# Find all emails, full name, phone number, address, ssn, linkedin extraction in the text\n",
    "emails = re.findall(email_pattern, resume_text)\n",
    "#phones = re.findall(us_phone_number_pattern, resume_text)\n",
    "#international_phones = re.findall(international_phone_pattern, resume_text)\n",
    "#us_addresses = re.findall(us_address_pattern, resume_text)\n",
    "#international_addresses = re.findall(international_address_pattern, resume_text, re.MULTILINE)\n",
    "#ssns = re.findall(social_security_number_pattern, resume_text)\n",
    "#linkedins = re.findall(linkedin_pattern, resume_text)\n",
    "\n",
    "# Print detected entities from NER\n",
    "entities = ner_pipeline(resume_text)\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "# entities extracted\n",
    "print(f\"Emails: {emails}\")\n",
    "#print(f\"US Phone Numbers: {phones}\")\n",
    "#print(f\"International Phone Numbers: {international_phones}\")\n",
    "#print(f\"US Addresses: {us_addresses}\")\n",
    "#print(f\"International Addresses: {international_addresses}\")\n",
    "#print(f\"Social Security Numbers: {ssns}\")\n",
    "#print(f\"LinkedIn Profiles: {linkedins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67a91a",
   "metadata": {},
   "source": [
    "# ner model entity redaction code & observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d09228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# NER models\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\") #BERT\n",
    "#ner_pipeline = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\") #RoBERTa\n",
    "#ner_pipeline = pipeline(\"ner\", model=\"albert/albert-base-v1\", aggregation_strategy=\"simple\") # ALBERT\n",
    "#ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\") #distilBERT\n",
    "\n",
    "\n",
    "\n",
    "def redact_pii(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    redacted_text = text\n",
    "    for entity in entities:\n",
    "# Add and remove '#' to test different entity for redaction\n",
    "        #if entity['entity_group'] in ['PER', 'ORG', 'LOC', 'MISC','GPE']:\n",
    "        #if entity['entity_group'] in ['MISC']:\n",
    "        #if entity['entity_group'] in ['PER']:\n",
    "        if entity['entity_group'] in ['ORG']:\n",
    "        #if entity['entity_group'] in ['LOC']:\n",
    "        #if entity['entity_group'] in ['GPE']:\n",
    "          redacted_text = redacted_text.replace(entity['word'], \"[REDACTED]\", 1)  # Replace only the first occurrence\n",
    "    \n",
    "    return redacted_text\n",
    "\n",
    "redacted_text_out = redact_pii(resume_text)\n",
    "\n",
    "print(redacted_text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ad5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "ner_pipeline = spacy.load(\"en_core_web_sm\") # Spacy\n",
    "\n",
    "\n",
    "\n",
    "def redact_pii(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    redacted_text = text\n",
    "    for entity in entities:\n",
    "# Add and remove '#' to test different entity for redaction\n",
    "        #if entity['entity_group'] in ['PER', 'ORG', 'LOC', 'MISC','GPE']:\n",
    "        #if entity['entity_group'] in ['MISC']:\n",
    "        #if entity['entity_group'] in ['PER']:\n",
    "        if entity['entity_group'] in ['ORG']:\n",
    "        #if entity['entity_group'] in ['LOC']:\n",
    "        #if entity['entity_group'] in ['GPE']:\n",
    "          redacted_text = redacted_text.replace(entity['word'], \"[REDACTED]\", 1)  # Replace only the first occurrence\n",
    "            \n",
    "def redact_pii(text):\n",
    "    doc = nlp(text)\n",
    "    redacted_text = text\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PER', 'ORG', 'LOC', 'MISC','GPE']:\n",
    "        #if ent.label_ in ['MISC']:\n",
    "        #if ent.label_ in ['PER']:\n",
    "        #if ent.label_ in ['ORG']:\n",
    "       # if ent.label_ in ['LOC']:\n",
    "       # if ent.label_ in ['GPE']:\n",
    "            redacted_text = redacted_text.replace(ent.text, \"[REDACTED]\", 1)  # Replace only the first occurrence\n",
    "    \n",
    "    return redacted_text\n",
    "\n",
    "redacted_text_out = redact_pii(resume_text)\n",
    "\n",
    "print(redacted_text_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aadc86",
   "metadata": {},
   "source": [
    "## Bert Entity Redaction Observations(Entire Resume):\n",
    "\n",
    "Alot of false positives for PERSON, ORG, LOC, MISC entities - These were mostely due to MISC entity\n",
    "Which entity had the most false positives\n",
    "\n",
    "- PERSON observations:What's the definition of this?  Seems to get Names but still captures false positives\n",
    "- MISC observations: How are these entities defined?  Is there a way to break this down for use in another PII\n",
    "- LOC observations: Captured some but not all\n",
    "- ORG observations: Captured some but not all\n",
    "- OVERALL observations: I'd use PERSON, LOC, & ORG FROM entities, improvements are needed\n",
    "\n",
    "\n",
    "NEXT STEPS: TEST WITH OTHER MODELS SUCH AS (ALBERT, DELBERT, SpaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f075c0d",
   "metadata": {},
   "source": [
    "## ROBERTA Entity Redaction Observations(Entire Resume):\n",
    "\n",
    "Alot of false positives for PERSON, ORG, LOC, MISC entities - These were mostely due to MISC entity\n",
    "Which entity had the most false positives\n",
    "\n",
    "- PERSON observations:\n",
    "- MISC observations:  \n",
    "- LOC observations: \n",
    "- ORG observations: \n",
    "- OVERALL observations:\n",
    "\n",
    "\n",
    "NEXT STEPS: TEST WITH OTHER MODELS SUCH AS (ALBERT, DELBERT, SpaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08098a9",
   "metadata": {},
   "source": [
    "## ALBERT Entity Redaction Observations(Entire Resume):\n",
    "\n",
    "Alot of false positives for PERSON, ORG, LOC, MISC entities - These were mostely due to MISC entity\n",
    "Which entity had the most false positives\n",
    "\n",
    "- PERSON observations:\n",
    "- MISC observations:  \n",
    "- LOC observations: \n",
    "- ORG observations: \n",
    "- OVERALL observations:\n",
    "\n",
    "\n",
    "NEXT STEPS: TEST WITH OTHER MODELS SUCH AS (ALBERT, DELBERT, SpaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e5a7c",
   "metadata": {},
   "source": [
    "## DISTILBERT Entity Redaction Observations(Entire Resume):\n",
    "\n",
    "\n",
    "- PERSON observations:\n",
    "- MISC observations:  \n",
    "- LOC observations: \n",
    "- ORG observations: \n",
    "- OVERALL observations:\n",
    "\n",
    "\n",
    "NEXT STEPS: TEST WITH OTHER MODELS SUCH AS (ALBERT, DELBERT, SpaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa6522",
   "metadata": {},
   "source": [
    "## SpaCy Entity Redaction Observations(Entire Resume):\n",
    "\n",
    "Alot of false positives for PERSON, ORG, LOC, MISC entities - These were mostely due to MISC entity\n",
    "Which entity had the most false positives\n",
    "\n",
    "- PERSON observations:\n",
    "- MISC observations:  \n",
    "- LOC observations: \n",
    "- ORG observations: \n",
    "- OVERALL observations:\n",
    "\n",
    "\n",
    "NEXT STEPS: TEST WITH OTHER MODELS SUCH AS (ALBERT, DELBERT, SpaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89ac3e",
   "metadata": {},
   "source": [
    "## Pattern Based Redaction Code & Observations (Entire Resume):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redactomg pii values based on regex pattern\n",
    "# Be consistent when running code for each pattern, add \n",
    "\n",
    "import re\n",
    "\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}' #email\n",
    "#us_phone_number_pattern = r'(\\+?1\\s?)?(\\(?\\d{3}\\)?[\\s-]?)?\\d{3}[\\s-]?\\d{4}'  #US phone number pattern\n",
    "#international_phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{1,4}?\\)?[-.\\s]?)?(\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9})'  #International phone number pattern\n",
    "#us_address_pattern = r'([0-9]+)\\s([A-Za-z0-9\\-\\s]+),\\s([A-Za-z\\s]+),\\s([A-Z]{2})\\s([0-9\\-]{5,10})'  #US address pattern\n",
    "#international_address_pattern = r'^\\d{1,5}\\s[\\w\\s.,\\'-]+,\\s*[\\w\\s.,\\'-]+,\\s*[A-Z]{2,3}\\s*\\d{2,5}(-\\d{4})?$'  #International address pattern\n",
    "#social_security_number_pattern = r'\\b\\d{3}-\\d{2}-\\d{4}\\b'  #Social Security Number pattern (XXX-XX-XXXX)\n",
    "#linkedin_pattern = r'\\b(?:https?://)?(?:www\\.)?linkedin\\.com/in/[a-zA-Z0-9._-]+\\b'  #LinkedIn profile pattern\n",
    "\n",
    "def redact_pattern_pii(text):\n",
    "    redacted_text = re.sub(email_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(us_phone_number_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(international_phone_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(us_address_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(international_address_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(social_security_number_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(linkedin_pattern, \"[REDACTED]\", text)\n",
    "    \n",
    "    return redacted_text\n",
    "\n",
    "redacted_text_out = redact_pattern_pii(resume_text)\n",
    "\n",
    "print(redacted_text_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b99ca3",
   "metadata": {},
   "source": [
    "## observations so far\n",
    "\n",
    "Phone Numbers:\n",
    "- incomplete phone numbers captured, tuples within parenthesis\n",
    "- refine regex pattern\n",
    "\n",
    "[\\+(9[976]\\d|8[987530]\\d|6[987]\\d|5[90]\\d|42\\d|3[875]\\d|\n",
    "2[98654321]\\d|9[8543210]|8[6421]|6[6543210]|5[87654321]|\n",
    "4[987654310]|3[9643210]|2[70]|7|1)\\d{1,14}$] - found on https://stackoverflow.com/questions/2113908/what-regular-expression-will-match-valid-international-phone-numbers\n",
    "- tried to \n",
    "\n",
    "Addresses:\n",
    "- incomplete as well\n",
    "\n",
    "https://stackoverflow.com/questions/55769704/regex-for-matching-a-single-line-standard-usps-address\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99d359",
   "metadata": {},
   "source": [
    "# Miscellenious Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d09641",
   "metadata": {},
   "source": [
    "## Entity Types in SpaCy: \n",
    "https://towardsdatascience.com/extend-named-entity-recogniser-ner-to-label-new-entities-with-spacy-339ee5979044\n",
    "* = relevent\n",
    "\n",
    "\n",
    "**PERSON - People, including fictional.\n",
    "\n",
    "ORP - Nationalities or religious or political groups.\n",
    "\n",
    "FAC - Buildings, airports, highways, bridges, etc.\n",
    "\n",
    "**ORG - Companies, agencies, institutions, etc.\n",
    "\n",
    "**GPE - Countries, cities, states. *\n",
    "\n",
    "**LOC - Non-GPE locations, mountain ranges, bodies of water. *\n",
    "\n",
    "PRODUCT - Objects, vehicles, foods, etc. (Not services.)\n",
    "\n",
    "EVENT - Named hurricanes, battles, wars, sports events, etc.\n",
    "\n",
    "WORK_OF_ART - Titles of books, songs, etc.\n",
    "\n",
    "LAW - Named documents made into laws.\n",
    "\n",
    "LANGUAGE - Any named language.\n",
    "\n",
    "DATE - Absolute or relative dates or periods.\n",
    "\n",
    "TIME - Times smaller than a day.\n",
    "\n",
    "PERCENT - Percentage, including \"%\".\n",
    "\n",
    "MONEY - Monetary values, including unit.\n",
    "\n",
    "QUANTITY - Measurements, as of weight or distance.\n",
    "\n",
    "ORDINAL - \"first\", \"second\", etc.\n",
    "\n",
    "CARDINAL - Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c96ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
