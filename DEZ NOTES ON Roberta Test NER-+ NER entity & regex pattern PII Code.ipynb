{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f340ad",
   "metadata": {},
   "source": [
    "# NICK'S ORIGINAL CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56375d",
   "metadata": {},
   "source": [
    "# OBJECTIVE: (TEAM MEETING NOTES)\n",
    "\n",
    "1. Complete EDA analysis on the base NER model. \n",
    "2. After we can begin experimenting with LLM models. \n",
    "3. Once we determine best model, we can move forward with explainability features that work best for our business use case. \n",
    "\n",
    "**Business use case: \n",
    "- individuals trying to explore the job market without industries paying for a first step forward. \n",
    "- A job searching tool that is completely unbiased from other companies, just looking forward with the individual’s best intention to aid in their job hunt. \n",
    "\n",
    "\n",
    "**FOR LATER:\n",
    "\n",
    "Idea: \n",
    "- use API from Bureau of Labor Statistics to use as baseline of occupations and industries (free open source data to extract from) \n",
    "- Job recommendations idea: use cluster analysis to give consumer “jobs you may enjoy if you want to move industries(industries as series, job title as title and if we want to reference NAICS directly, we can reference the 6 digit NAICS code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39244000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "#from docx import Document\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pdfplumber\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import os\n",
    "import json\n",
    "#from docx import Document\n",
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from plotly.offline import plot\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e044a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a22515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cfc265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b32b0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54816c",
   "metadata": {},
   "source": [
    "## Loading the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = \"\"\"\n",
    "\n",
    "Nicholas E. Reese\n",
    "Van Ness, Washington D.C.\n",
    "412-216-2398\n",
    "Nicholas.E.Reese15@gmail.com \n",
    "Education \n",
    "•Georgetown University - August 2023 – December 2024\n",
    "Master of Science Business Analytics – MSBA candidate\n",
    "Peer-Elected Class Representative \n",
    "•Dickinson College - August 2014 – September 2018\n",
    "Economics & Political Science Double Major\n",
    "Varsity Tennis Captain \n",
    "•University of Bologna - August 2016\n",
    "\n",
    "Skills\n",
    "- R studio\n",
    "- Python\n",
    "- Power BI\n",
    "- SQL\n",
    "- Neural Networks\n",
    "- Machine Learning\n",
    "- Macro Modeling\n",
    "- AWS Cloud Services\n",
    "- Pandas\n",
    "- A/B Testing\n",
    "- Scikit-Learn\n",
    "- Econometrics\n",
    "\n",
    "Professional Experience\n",
    "FINRA, Washington, D.C.\tJune 2020 – Present \n",
    "Senior Analyst, Market Regulation\tSeptember 2022 – Present \n",
    "•Earned top 10% of performance of analysts for past two years.\n",
    "•Led team in research implementing PostgreSQL and NoSQL queries for large data pulls.\n",
    "•Spearheaded new analytical approaches to financial workflow with tools such as R & Python for model development.\n",
    "•Developed working predictive modeling schemas for senior staff using statistical analysis.\n",
    "•Leveraged skills in platforms like Python, R, Power BI, Tableau and SQL accompanied with strong statistical background in financial markets.\n",
    "•Built the Security-Based Swap training manual deck & produced the recorded info session for all of FINRA. \n",
    "•Selected for the FINRA’s first Georgetown Advanced Analytics Program as one of the most junior staff awarded opportunity.\n",
    "•Incorporated statistical analytics to assist in creating a NPL tool to analyze financial documents language to minimize time on manual analysis.\n",
    "•Produced unsupervised and supervised models to perform analysis for Security-Based Swaps trade patterns.\n",
    "•Improved FINRA platforms with Data Scientists for higher accuracy and efficiency. \n",
    "•Managed process of re-engineering supervisory reviews through advanced analytic tools for senior staff.\n",
    "•Implements advanced analytics to assist senior staff with maximizing efficiencies in daily workloads and trade pattern creations.\n",
    "•Presented visualization case work using Power BI & Tableau to senior leadership.\n",
    "•Persuaded senior staff to allow for statistical analytics tools like R.\n",
    "•Mentored over 20 junior & senior staff members on improving processes with analytical tools for financial reviews.\n",
    "•Managed junior staff with day-to-day workload while teaching the staff how to use advanced analytics.\n",
    "Analyst, Market Regulation\tJune 2021-September 2022\n",
    "·Created macroeconomic models for newly implemented FINRA Rule 2232 and MSRB Rule G-15.\n",
    "·Mentored junior staff with creating macro models, synthesizing responses from FINRA member firms.\n",
    "·Won the Regulator Scholarship for continued financial learning based on individual performance.\n",
    "·Received six internal awards for expertise in Municipal and Corporate Bond analysis as an analyst.\n",
    "Associate Analyst, Market Regulation\tJune 2020 – June 2021\n",
    "·Created macroeconomic models to quantify business models of selected firms for FINRA Rule 2232.\n",
    "             BNY Mellon, Pittsburgh, PA\t\t\t\t\t\t                 September 2019 – June 2020\n",
    "Corporate Trust Associate\n",
    "·Leader & instructor of the Bloomberg software for the Corporate Trust Team.\n",
    "LendingHome, Internship, Pittsburgh, PA\n",
    "Funding Specialist & Post Closing Member\tMarch 2019 – August 2019\n",
    "·Reviewed closing documents to ensure precise execution for funding staff.\n",
    "              Veraction/Trax, Junior Business Analyst, Memphis, TN\t  June 2016 - August 2016                                                    June 2016 - August 2016\n",
    "·Led the creation, development, and implementation of Request For Proposal (RFP) database.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16774b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf437695",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d14c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline('ner', model = \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "                       aggregation_strategy = 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ner_pipeline(resume_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b34555a",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_entities = [entity for entity in entities if entity['entity_group'] != 'MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011758b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in filtered_entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21cd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern for email extraction\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "\n",
    "# Find all email addresses in the text\n",
    "emails = re.findall(email_pattern, resume_text)\n",
    "\n",
    "# Display found email addresses\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98760ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the NER pipeline\n",
    "ner_pipeline2 = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "\n",
    "# Detect entities using the NER pipeline\n",
    "entities2 = ner_pipeline2(resume_text)\n",
    "\n",
    "# Regex pattern for email extraction\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "\n",
    "# Find all email addresses in the text\n",
    "emails = re.findall(email_pattern, resume_text)\n",
    "\n",
    "# Print detected entities from NER\n",
    "for entity in entities2:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70122958",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_test = \"\"\"\n",
    "Nicholas E. Reese\n",
    "Van Ness, Washington D.C.\n",
    "412-216-2398\n",
    "Nicholas.E.Reese@gmail.com\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c82d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ner_pipeline(resume_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb81034",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242545e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds ={\n",
    "    'PER': .7,\n",
    "    'ORG': .8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "redacted_text = resume_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entities:\n",
    "    entity_type = entity['entity_group']\n",
    "    score = entity['score']\n",
    "    \n",
    "    if entity_type == entity and score >= thresholds[entity]:\n",
    "        entity_text = entity['word']\n",
    "        redacted_text = re.sub(re.escape(entity_text), '[Redacted]', redacted_text)\n",
    "\n",
    "# Redact the email addresses\n",
    "redacted_text = re.sub(r'\\S+@\\S+', '[Redacted Email]', redacted_text)\n",
    "\n",
    "\n",
    "\n",
    "print(redacted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb93e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(redacted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_test = \"\"\"\n",
    "Nicholas E. Reese\n",
    "Van Ness, Washington D.C.\n",
    "412-216-2398\n",
    "Nicholas.E.Reese@gmail.com\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d55b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ner_pipeline(resume_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a740e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds ={\n",
    "    'PER': .8,\n",
    "    'ORG': .9,\n",
    "    'LOC': .8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "redacted_text = resume_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_sorted = sorted(entities, key=lambda x: len(x['word']), reverse=True)\n",
    "\n",
    "\n",
    "for entity in entities_sorted:\n",
    "    entity_type = entity['entity_group']\n",
    "    score = entity['score']\n",
    "    \n",
    "    if entity_type == entity_type and score >= thresholds[entity_type]:\n",
    "        entity_text = entity['word']\n",
    "        redacted_text = re.sub(re.escape(entity_text), '[Redacted]', redacted_text)\n",
    "\n",
    "\n",
    "        \n",
    "for entity in entities_sorted:\n",
    "    entity_type = entity['entity_group']\n",
    "    score = entity['score']\n",
    "    \n",
    "    if entity_type in thresholds and score >= thresholds[entity_type]:\n",
    "        entity_text = entity['word']\n",
    "        # Make sure to redact the smaller parts again if needed\n",
    "        redacted_text = re.sub(re.escape(entity_text), '[Redacted]', redacted_text)\n",
    "\n",
    "# Redact the email addresses\n",
    "redacted_text = re.sub(r'\\S+@\\S+', '[Redacted Email]', redacted_text)\n",
    "redacted_text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[Redacted Phone]', redacted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7099004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(redacted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5ab2e",
   "metadata": {},
   "source": [
    "# Dezmond Initial Reactions\n",
    "\n",
    "So far it looks like a single resume has been used for extracting entities and identifying emails.  The model that was used was NER Bert Model.  I think this was done to ensure each step worked before moving forward.\n",
    "\n",
    "The process seems straight forward. But I'd like more resumes for exposure to different resume format, styles, entities and pii values.\n",
    "\n",
    "For the sake of this exercise, I will do the following:\n",
    "- add more regex patterns to help extract pii not captured in entities\n",
    "- use other ner models to detect pii with entities\n",
    "- MAYBE compare ner model results\n",
    "\n",
    "I will make sure not to use a code editing tool to clean up my code or comments as to keep my points and intentions straightforward\n",
    "\n",
    "## What Pii Values do resumes have that need to be identified\n",
    "- phone number\n",
    "- website link (linkedin, social media, git links)\n",
    "- address\n",
    "- name\n",
    "- unique identifiers (tax id #, social security #, etc...)\n",
    "- international identifiers should be considered (Mentioned in Deloitte Meeting)\n",
    "\n",
    "\n",
    "## If Pii is a concern, how do we get rid or at least hide it\n",
    "- redact\n",
    "- replace\n",
    "\n",
    "\n",
    "## What other EDA can I do?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Other Notes,\n",
    "\n",
    "I recoemmedn keeping a running list of all sites where you pull code from directly or reference.  I will add linked to sites i directly reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a103658",
   "metadata": {},
   "source": [
    "# Regex patterns and ner model apis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa111f",
   "metadata": {},
   "source": [
    "## Other regex patterns for pii values in resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acd624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#us_full_name_pattern = r'\\b[A-Z][a-z]*\\s[A-Z][a-z]*\\b',  # pattern for simpler full name (e.g., John Doe)\n",
    "#international_full_name =\n",
    "\n",
    "us_phone_number = r'(\\+?1\\s?)?(\\(?\\d{3}\\)?[\\s-]?)?\\d{3}[\\s-]?\\d{4}',  # US phone # pattern\n",
    "international_phone = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{1,4}?\\)?[-.\\s]?)?(\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9})'\n",
    "#us_address = r'\\d{1,4}\\s\\w+\\s\\w+\\.?',  # street address ('123 Main St) '' - Legacy\n",
    "us_address = r'([0-9]+)\\s([A-Za-z0-9\\-\\s]+),\\s([A-Za-z\\s]+),\\s([A-Z]{2})\\s([0-9\\-]{5,10})'\n",
    "international_address = r'^\\d{1,5}\\s[\\w\\s.,\\'-]+,\\s*[\\w\\s.,\\'-]+,\\s*[A-Z]{2,3}\\s*\\d{2,5}(-\\d{4})?$'\n",
    "social_security_number = r'\\b\\d{3}-\\d{2}-\\d{4}\\b',  # Social security number pattern (XXX-XX-XXXX)\n",
    "linkedin = r'\\b[a-zA-Z0-9._-]+/in/[a-zA-Z0-9._-]+\\b',  # LinkedIn profile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286c7cc",
   "metadata": {},
   "source": [
    "## Other ner models for detecting pii with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfe1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlm-roberta-large-NER\n",
    "# https://huggingface.co/51la5/roberta-large-NER\n",
    "# The model is a language model. The model can be used for token classification, a natural language understanding task in which a label is assigned to some tokens in a text.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#albert-base-v2-finetuned-ner\n",
    "#https://huggingface.co/Jorgeutd/albert-base-v2-finetuned-ner\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-base-NER\n",
    "# https://huggingface.co/dslim/bert-base-NER\n",
    "# bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "ner_pipeline2 = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70dd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilbert-NER\n",
    "# https://huggingface.co/dslim/distilbert-NER\n",
    "# smaller, faster, more efficent variant of BERT\n",
    "# distilbert-NER is the fine-tuned version of DistilBERT, which is a distilled variant of the BERT model. DistilBERT has fewer parameters than BERT, making it smaller, faster, and more efficient. distilbert-NER is specifically fine-tuned for the task of Named Entity Recognition (NER).\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert-large-NER\n",
    "# https://huggingface.co/dslim/bert-large-NER\n",
    "# bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65451969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy\n",
    " # https://ubiai.tools/fine-tuning-spacy-models-customizing-named-entity-recognition/\n",
    "# As an open-source library, SpaCy provides pre-trained models for essential tasks like part-of-speech tagging, named entity recognition, and dependency parsing.\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "ner_pipeline = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170219c",
   "metadata": {},
   "source": [
    "## ner model & regex pii identfying code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at how well each ner model or regex pattern identified pii values\n",
    "# Maybe figureout if we can set a threshold score for classifying entities .75 and up maybe...\n",
    "\n",
    "\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "\n",
    "\n",
    "# NER MODELS\n",
    "\n",
    "#albert-base-v2-finetuned-ner\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "\n",
    "# xlm-roberta-large-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# bert-base-NER\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "# distilbert-NER\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "# # bert-large-NER\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "\n",
    "# spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "ner_pipeline = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "# PIPELINE\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Finding entities using the NER pipeline\n",
    "entities = ner_pipeline(resume_text)\n",
    "\n",
    "# Regex pattern for email, full name, phone number, address, ssn, linkedin extraction\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "#us_phone_number_pattern = r'(\\+?1\\s?)?(\\(?\\d{3}\\)?[\\s-]?)?\\d{3}[\\s-]?\\d{4}'  # US phone number pattern\n",
    "#international_phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{1,4}?\\)?[-.\\s]?)?(\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9})'  # International phone number pattern\n",
    "#us_address_pattern = r'([0-9]+)\\s([A-Za-z0-9\\-\\s]+),\\s([A-Za-z\\s]+),\\s([A-Z]{2})\\s([0-9\\-]{5,10})'  # US address pattern\n",
    "#international_address_pattern = r'^\\d{1,5}\\s[\\w\\s.,\\'-]+,\\s*[\\w\\s.,\\'-]+,\\s*[A-Z]{2,3}\\s*\\d{2,5}(-\\d{4})?$'  # International address pattern\n",
    "#social_security_number_pattern = r'\\b\\d{3}-\\d{2}-\\d{4}\\b'  # Social Security Number pattern (XXX-XX-XXXX)\n",
    "#linkedin_pattern = r'\\b(?:https?://)?(?:www\\.)?linkedin\\.com/in/[a-zA-Z0-9._-]+\\b'  # LinkedIn profile pattern\n",
    "\n",
    "\n",
    "# Find all emails, full name, phone number, address, ssn, linkedin extraction in the text\n",
    "emails = re.findall(email_pattern, resume_text)\n",
    "#phones = re.findall(us_phone_number_pattern, resume_text)\n",
    "#international_phones = re.findall(international_phone_pattern, resume_text)\n",
    "#us_addresses = re.findall(us_address_pattern, resume_text)\n",
    "#international_addresses = re.findall(international_address_pattern, resume_text, re.MULTILINE)\n",
    "#ssns = re.findall(social_security_number_pattern, resume_text)\n",
    "#linkedins = re.findall(linkedin_pattern, resume_text)\n",
    "\n",
    "# Print detected entities from NER\n",
    "entities = ner_pipeline(resume_text)\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "# print entities extracted\n",
    "print(f\"Emails: {emails}\")\n",
    "#print(f\"US Phone Numbers: {phones}\")\n",
    "#print(f\"International Phone Numbers: {international_phones}\")\n",
    "#print(f\"US Addresses: {us_addresses}\")\n",
    "#print(f\"International Addresses: {international_addresses}\")\n",
    "#print(f\"Social Security Numbers: {ssns}\")\n",
    "#print(f\"LinkedIn Profiles: {linkedins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a491cdc",
   "metadata": {},
   "source": [
    "# ner model entity redaction code & observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07084fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# NER MODELS\n",
    "\n",
    "#albert-base-v2-finetuned-ner\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "\n",
    "# xlm-roberta-large-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# bert-base-NER\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "# distilbert-NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "# bert-large-NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "\n",
    "# spacy\n",
    "#nlp = spacy.load(\"en_core_web_lg\") \n",
    "#ner_pipeline = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "# PIPELINE\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "def redact_pii(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    redacted_text = text\n",
    "    for entity in entities:\n",
    "        #if entity['entity'] in ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']:\n",
    "        if entity['entity'] in ['I-PER', 'I-LOC', 'I-ORG']:\n",
    "        #if entity['entity'] in ['B-PER', 'I-PER']:\n",
    "        #if entity['entity'] in ['B-LOC', 'I-LOC']:\n",
    "        #if entity['entity'] in ['B-ORG', 'I-ORG']:\n",
    "            start, end = entity['start'], entity['end']\n",
    "            redacted_text = redacted_text[:start] + \"[REDACTED]\" + redacted_text[end:]\n",
    "    \n",
    "    return redacted_text\n",
    "\n",
    "redacted_text_out = redact_pii(resume_text)\n",
    "\n",
    "print(redacted_text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b399fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "ner_pipeline = spacy.load(\"en_core_web_lg\") # Spacy\n",
    "\n",
    "\n",
    "\n",
    "def redact_pii(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    redacted_text = text\n",
    "    for entity in entities:\n",
    "# Add and remove '#' to test different entity for redaction\n",
    "        if entity['entity_group'] in ['PERSON', 'LOC','GPE', 'ORG']:\n",
    "        #if entity['entity_group'] in ['MISC']:\n",
    "        #if entity['entity_group'] in ['PERSON']:\n",
    "        #if entity['entity_group'] in ['ORG']:\n",
    "        #if entity['entity_group'] in ['LOC']:\n",
    "        #if entity['entity_group'] in ['GPE']:\n",
    "          redacted_text = redacted_text.replace(entity['word'], \"[REDACTED]\", 1)  # Replace only the first occurrence\n",
    "            \n",
    "def redact_pii(text):\n",
    "    doc = nlp(text)\n",
    "    redacted_text = text\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PERSON', 'LOC','GPE']:\n",
    "        #if ent.label_ in ['MISC']:\n",
    "       # if ent.label_ in ['PERSON']:\n",
    "        #if ent.label_ in ['ORG']:\n",
    "       # if ent.label_ in ['LOC']:\n",
    "       # if ent.label_ in ['GPE']:\n",
    "            redacted_text = redacted_text.replace(ent.text, \"[REDACTED]\", 1)  # Replace only the first occurrence\n",
    "    \n",
    "    return redacted_text\n",
    "\n",
    "redacted_text_out = redact_pii(resume_text)\n",
    "\n",
    "print(redacted_text_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1761028",
   "metadata": {},
   "source": [
    "# Testing NER Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd0e03c",
   "metadata": {},
   "source": [
    "# More Dezmond Reactions\n",
    "\n",
    "- So far I have performed test with 6 ner models\n",
    "- It should be noted entity labels are different due to the NER models used ie per entity is broken down into i-per and b-per\n",
    "- Spacy appears to be the cleanest when identifying and redacting entities (person, org, location) from a test\n",
    "- All models appear succesffully redact entities but there are quite a few with multiple intersecting redact labels.  This needs to fixed so only redacted text replaced the entity or pii value\n",
    "- All the thresholds were left at .75 for the tests.\n",
    "- Dezmond's resume include linked, so I will test out the regex linked redacter\n",
    "\n",
    "\n",
    "Next Steps:\n",
    "- I will perform tests on the corpus (all saxa 4 resumes)\n",
    "- I will add regex patterns for pii values not covered by ner enties\n",
    "- I have to find a way to clean up the multiple redacted inputs when redacting entities\n",
    "\n",
    "\n",
    "- If I had to pick a model today, I'd say spacy with Bert-large-NeR due to the cleanliness of the redacted outputs in tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# NER MODELS\n",
    "\n",
    "#albert-base-v2-finetuned-ner\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "\n",
    "# xlm-roberta-large-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# bert-base-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "# distilbert-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "# # bert-large-NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "\n",
    "# spacy\n",
    "#nlp = spacy.load(\"en_core_web_lg\") \n",
    "#ner_pipeline = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "# PIPELINE\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_test = \"\"\"\n",
    "Nicholas E. Reese\n",
    "Van Ness, Washington D.C.\n",
    "412-216-2398\n",
    "Nicholas.E.Reese@gmail.com\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a02a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ner_pipeline(resume_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70291a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'B-PER': 0.75,\n",
    "    'I-PER': 0.75,\n",
    "    'B-ORG': 0.75,\n",
    "    'I-ORG': 0.75,\n",
    "    'B-LOC': 0.75,\n",
    "    'I-LOC': 0.75,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed38ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "redacted_text = resume_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d53248",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_sorted = sorted(entities, key=lambda x: len(x['word']), reverse=True)\n",
    "\n",
    "\n",
    "for entity in entities_sorted:\n",
    "    entity_type = entity['entity']\n",
    "    score = entity['score']\n",
    "    \n",
    "    if entity_type == entity_type and score >= thresholds[entity_type]:\n",
    "        entity_text = entity['word']\n",
    "        redacted_text = re.sub(re.escape(entity_text), '[Redacted]', redacted_text)\n",
    "\n",
    "\n",
    "        \n",
    "for entity in entities_sorted:\n",
    "    entity_type = entity['entity']\n",
    "    score = entity['score']\n",
    "    \n",
    "    if entity_type in thresholds and score >= thresholds[entity_type]:\n",
    "        entity_text = entity['word']\n",
    "        # Make sure to redact the smaller parts again if needed\n",
    "        redacted_text = re.sub(re.escape(entity_text), '[Redacted]', redacted_text)\n",
    "\n",
    "# Redact the email addresses\n",
    "redacted_text = re.sub(r'\\S+@\\S+', '[Redacted Email]', redacted_text)\n",
    "redacted_text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[Redacted Phone]', redacted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(redacted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redactomg pii values based on regex pattern\n",
    "# Be consistent when running code for each pattern, add \n",
    "\n",
    "import re\n",
    "\n",
    "#email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}' #email\n",
    "us_phone_number_pattern = r'(\\+?1\\s?)?(\\(?\\d{3}\\)?[\\s-]?)?\\d{3}[\\s-]?\\d{4}'  #US phone number pattern\n",
    "#international_phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?(\\(?\\d{1,4}?\\)?[-.\\s]?)?(\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9})'  #International phone number pattern\n",
    "#us_address_pattern = r'([0-9]+)\\s([A-Za-z0-9\\-\\s]+),\\s([A-Za-z\\s]+),\\s([A-Z]{2})\\s([0-9\\-]{5,10})'  #US address pattern\n",
    "#international_address_pattern = r'^\\d{1,5}\\s[\\w\\s.,\\'-]+,\\s*[\\w\\s.,\\'-]+,\\s*[A-Z]{2,3}\\s*\\d{2,5}(-\\d{4})?$'  #International address pattern\n",
    "#social_security_number_pattern = r'\\b\\d{3}-\\d{2}-\\d{4}\\b'  #Social Security Number pattern (XXX-XX-XXXX)\n",
    "#linkedin_pattern = r'\\b(?:https?://)?(?:www\\.)?linkedin\\.com/in/[a-zA-Z0-9._-]+\\b'  #LinkedIn profile pattern\n",
    "\n",
    "def redact_pattern_pii(text):\n",
    "    #redacted_text = re.sub(email_pattern, \"[REDACTED]\", text)\n",
    "    redacted_text = re.sub(us_phone_number_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(international_phone_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(us_address_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(international_address_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(social_security_number_pattern, \"[REDACTED]\", text)\n",
    "   # redacted_text = re.sub(linkedin_pattern, \"[REDACTED]\", text)\n",
    "    \n",
    "    return redacted_text\n",
    "\n",
    "redacted_text_out = redact_pattern_pii(resume_text)\n",
    "\n",
    "print(redacted_text_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98db643",
   "metadata": {},
   "source": [
    "# Test ner models on corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d938962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848be7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "def convert_files_to_json(folder_path, output_json_file):\n",
    "    corpus = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Skip temporary files created by Word\n",
    "        if filename.startswith('~$'):\n",
    "            print(f\"Skipped temporary file: {filename}\")\n",
    "            continue\n",
    "        \n",
    "        if filename.endswith('.docx'):\n",
    "            try:\n",
    "                doc = Document(file_path)\n",
    "                text = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "                document_data = {\n",
    "                    'title': filename,\n",
    "                    'text': text, \n",
    "                    'type': 'word',\n",
    "                    'file_path': file_path\n",
    "                }\n",
    "                corpus.append(document_data)\n",
    "                print(f\"Added .docx: {filename}\") \n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing .docx file {filename}: {e}\")\n",
    "        \n",
    "        elif filename.endswith('.pdf'):\n",
    "            try:\n",
    "                with pdfplumber.open(file_path) as pdf:\n",
    "                    text = '\\n'.join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "                    document_data = {\n",
    "                        'title': filename, \n",
    "                        'text': text,\n",
    "                        'type': 'pdf',\n",
    "                        'file_path': file_path\n",
    "                    }\n",
    "                    corpus.append(document_data)\n",
    "                    print(f\"Added .pdf: {filename}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing .pdf file {filename}: {e}\")\n",
    "                \n",
    "    with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(corpus, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Call the function with your paths\n",
    "convert_files_to_json(r'C:\\Users\\dezri\\Desktop\\GRAD.MSBA.PROGRAM\\Capstone', 'output_corpus.json')\n",
    "# change your file path to where you saved the resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fd7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = pd.read_json('output_corpus.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = pd.DataFrame(resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88c65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# NER MODELS\n",
    "\n",
    "#albert-base-v2-finetuned-ner\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "\n",
    "# xlm-roberta-large-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# bert-base-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "# distilbert-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "# # bert-large-NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "\n",
    "# PIPELINE\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c66fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce7753",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in resumes.iterrows():\n",
    "    document_title = row['title']\n",
    "    document_text = row['text']\n",
    "    document_type = row['type']\n",
    "    document_path = row['file_path']\n",
    "\n",
    "    entities = ner_pipeline(document_text)\n",
    "\n",
    "    document_data = {\n",
    "        'title': document_title,\n",
    "        'text': document_text,\n",
    "        'type': document_type,\n",
    "        'file_path': document_path,\n",
    "        'entities': entities\n",
    "    }\n",
    "\n",
    "    structured_documents.append(document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in structured_documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'B-PER': 0.75,\n",
    "    'I-PER': 0.75,\n",
    "    'B-ORG': 0.75,\n",
    "    'I-ORG': 0.75,\n",
    "    'B-LOC': 0.75,\n",
    "    'I-LOC': 0.75,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_redacted_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10bc986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, document in enumerate(structured_documents):\n",
    "    original_text = document['text']\n",
    "    redacted_text = original_text\n",
    "    entities_sorted = sorted(document.get('entities', []), key=lambda x: len(x['word']), reverse=True)\n",
    "\n",
    "    print(f\"Processing Document {i + 1}: {document['title']}\")\n",
    "    print(f\"Entities from document {i + 1}:\")\n",
    "    \n",
    "    for entity in entities_sorted:\n",
    "        print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "    for entity in entities_sorted:\n",
    "        entity_type = entity['entity']\n",
    "        score = entity['score']\n",
    "\n",
    "        if entity_type in thresholds and score >= thresholds[entity_type]:\n",
    "            entity_text = entity['word']\n",
    "            redacted_text = re.sub(re.escape(entity_text), '[Redacted]', redacted_text)\n",
    "\n",
    "    redacted_text = re.sub(r'\\S+@\\S+', '[Redacted Email]', redacted_text)\n",
    "    redacted_text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[Redacted Phone]', redacted_text)\n",
    "    redacted_text = re.sub(r'\\b(?:https?://)?(?:www\\.)?linkedin\\.com/in/[a-zA-Z0-9._-]+\\b', '[Redacted Website]', redacted_text)\n",
    "\n",
    "    corpus_redacted_text.append(redacted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_redacted_text.append(redacted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dfa854",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of redacted documents: {len(corpus_redacted_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(corpus_redacted_text):\n",
    "    print(f\"Redacted Text for Document {i + 1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475c728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if document_index < len(corpus_redacted_text):\n",
    "    print(f\"Redacted Text for Document 1:\")\n",
    "    print(corpus_redacted_text[document_index])\n",
    "else:\n",
    "    print(\"Document does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106e3c8",
   "metadata": {},
   "source": [
    "## spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05757958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "def convert_files_to_json(folder_path, output_json_file):\n",
    "    corpus = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Skip temporary files created by Word\n",
    "        if filename.startswith('~$'):\n",
    "            print(f\"Skipped temporary file: {filename}\")\n",
    "            continue\n",
    "        \n",
    "        if filename.endswith('.docx'):\n",
    "            try:\n",
    "                doc = Document(file_path)\n",
    "                text = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "                document_data = {\n",
    "                    'title': filename,\n",
    "                    'text': text, \n",
    "                    'type': 'word',\n",
    "                    'file_path': file_path\n",
    "                }\n",
    "                corpus.append(document_data)\n",
    "                print(f\"Added .docx: {filename}\") \n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing .docx file {filename}: {e}\")\n",
    "        \n",
    "        elif filename.endswith('.pdf'):\n",
    "            try:\n",
    "                with pdfplumber.open(file_path) as pdf:\n",
    "                    text = '\\n'.join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "                    document_data = {\n",
    "                        'title': filename, \n",
    "                        'text': text,\n",
    "                        'type': 'pdf',\n",
    "                        'file_path': file_path\n",
    "                    }\n",
    "                    corpus.append(document_data)\n",
    "                    print(f\"Added .pdf: {filename}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing .pdf file {filename}: {e}\")\n",
    "                \n",
    "    with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(corpus, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Call the function with your paths\n",
    "convert_files_to_json(r'C:\\Users\\dezri\\Desktop\\GRAD.MSBA.PROGRAM\\Capstone', 'output_corpus.json')\n",
    "# change your file path to where you saved the resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = pd.read_json('output_corpus.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b99fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = pd.DataFrame(resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b419399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each resume and extract entities\n",
    "for index, row in resumes.iterrows():\n",
    "    document_title = row['title']\n",
    "    document_text = row['text']\n",
    "    document_type = row['type']\n",
    "    document_path = row['file_path']\n",
    "\n",
    "    # Process the text with SpaCy\n",
    "    doc = nlp(document_text)\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = [{'word': ent.text, 'entity': ent.label_, 'score': .9} for ent in doc.ents]  # Assigning a score of 1.0 for simplicity\n",
    "\n",
    "    document_data = {\n",
    "        'title': document_title,\n",
    "        'text': document_text,\n",
    "        'type': document_type,\n",
    "        'file_path': document_path,\n",
    "        'entities': entities  # This will hold the detected entities\n",
    "    }\n",
    "\n",
    "    structured_documents.append(document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332eafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total documents in structured_documents: {len(structured_documents)}\")\n",
    "for doc in structured_documents:\n",
    "    print(f\"Title: {doc['title']}, Entities Count: {len(doc['entities'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_redacted_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2050e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'PERSON': 0.75,\n",
    "    'ORG': 0.75,\n",
    "    'GPE': 0.75, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Code with Thresholds\n",
    "for i, document in enumerate(structured_documents):\n",
    "    original_text = document['text']\n",
    "    redacted_text = original_text\n",
    "    entities_sorted = sorted(document.get('entities', []), key=lambda x: len(x['word']), reverse=True)\n",
    "\n",
    "    print(f\"Processing Document {i + 1}: {document['title']}\")\n",
    "    print(f\"Entities from document {i + 1}:\")\n",
    "    \n",
    "    for entity in entities_sorted:\n",
    "        entity_text = entity['word']\n",
    "        entity_label = entity['entity']\n",
    "        score = entity.get('score', 1.0)\n",
    "    \n",
    "    if entity_label in thresholds and score >= thresholds[entity_label]:\n",
    "        print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "    for entity in entities_sorted:\n",
    "        entity_text = entity['word']\n",
    "        redacted_text = re.sub(re.escape(entity_text), '[Redacted]', redacted_text)\n",
    "\n",
    "    redacted_text = re.sub(r'\\S+@\\S+', '[Redacted Email]', redacted_text)\n",
    "    redacted_text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[Redacted Phone]', redacted_text)\n",
    "    redacted_text = re.sub(r'\\b(?:https?://)?(?:www\\.)?linkedin\\.com/in/[a-zA-Z0-9._-]+\\b', '[Redacted Website]', redacted_text)\n",
    "\n",
    "    corpus_redacted_text.append(redacted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of redacted documents: {len(corpus_redacted_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1efe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(corpus_redacted_text):\n",
    "    print(f\"Redacted Text for Document {i + 1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d7872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if document_index < len(corpus_redacted_text):\n",
    "    print(f\"Redacted Text for Document 1:\")\n",
    "    print(corpus_redacted_text[document_index])\n",
    "else:\n",
    "    print(\"Document does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563be65",
   "metadata": {},
   "source": [
    "# Testing Larger Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01593553",
   "metadata": {},
   "source": [
    "# More Dezmond Notes After Testing Larger Corpus\n",
    "\n",
    "- I added thresholds for the spacy, these are at .75\n",
    "- In spacy I switched from LOC to GPE entity because of the difference, below are definitions:\n",
    "\n",
    "GPE: geopolitical entities, e.g. everything with a governing body like cities and countries. Examples: \"Germany\", \"Buenos Aires\".\n",
    "LOC: everything else that's a physical location or area, like \"Kalahari Desert\" or \"Silicon Valley\"\n",
    "\n",
    "- when testing the larger corpus, I had to limit the amount of resumes for the redaction process.  I will run overnight to see if I can get the entire dataset redacted.  As long as you select the corpus size, the model should run\n",
    "\n",
    "- Please be advised Python is case sensitive, and jupyter notebook has shortcut for cutting cells (x), I messed up my code but I fixed it\n",
    "\n",
    "- both bert ner large and spacy redact entities fairly well\n",
    "- the email and phone regex patterns seems to do a decent job redacting\n",
    "- I believe this is in a decent enough state to hand it off.\n",
    "- I added code to export the redacted output into a corpus dataframe with the original id, category, and redacted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3378ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = pd.read_csv('Resumes_with_PII_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust this number to scale the size of the corpus\n",
    "resumes = resumes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef200426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a3649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# NER MODELS\n",
    "\n",
    "#albert-base-v2-finetuned-ner\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"Jorgeutd/albert-base-v2-finetuned-ner\")\n",
    "\n",
    "# xlm-roberta-large-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "#classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# bert-base-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "# distilbert-NER\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "# # bert-large-NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "\n",
    "# PIPELINE\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_documents_ner_corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d17ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in resumes.iterrows():\n",
    "    document_ID = row['ID']\n",
    "    document_Category = row['Category']\n",
    "    document_text = row['text']\n",
    "\n",
    "    entities = ner_pipeline(document_text)\n",
    "\n",
    "    document_data = {\n",
    "        'ID': document_ID,\n",
    "        'Category': document_Category,\n",
    "        'text': document_text,\n",
    "        'entities': entities  \n",
    "    }\n",
    "\n",
    "    structured_documents_ner_corpus.append(document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in structured_documents_ner_corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'B-PER': 0.75,\n",
    "    'I-PER': 0.75,\n",
    "    'B-ORG': 0.75,\n",
    "    'I-ORG': 0.75,\n",
    "    'B-LOC': 0.75,\n",
    "    'I-LOC': 0.75,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_corpus_redacted_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8857477",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, document in enumerate(structured_documents_ner_corpus):\n",
    "    original_text = document['text']\n",
    "    redacted_text = original_text\n",
    "    entities_sorted = sorted(document.get('entities', []), key=lambda x: len(x['word']), reverse=True)\n",
    "\n",
    "    print(f\"Processing Document {i + 1}: {document['ID']}\")\n",
    "    print(f\"Entities from document {i + 1}:\")\n",
    "    \n",
    "    for entity in entities_sorted:\n",
    "        entity_text = entity['word']\n",
    "        entity_label = entity['entity']\n",
    "        score = entity.get('score', 1.0)\n",
    "    \n",
    "    if entity_label in thresholds and score >= thresholds[entity_label]:\n",
    "        print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "    for entity in entities_sorted:\n",
    "        entity_text = entity['word']\n",
    "        redacted_text = re.sub(re.escape(entity_text), '[Redacted]', redacted_text)\n",
    "\n",
    "    redacted_text = re.sub(r'\\S+@\\S+', '[Redacted Email]', redacted_text)\n",
    "    redacted_text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[Redacted Phone]', redacted_text)\n",
    "    redacted_text = re.sub(r'\\b(?:https?://)?(?:www\\.)?linkedin\\.com/in/[a-zA-Z0-9._-]+\\b', '[Redacted Website]', redacted_text)\n",
    "    \n",
    "    \n",
    "    ner_corpus_redacted_text.append({\n",
    "        'ID': row['ID'],                #row instead of document to build seperate columns in output\n",
    "        'Category': row['Category'],    \n",
    "        'Redacted Text': redacted_text   \n",
    "    })\n",
    "\n",
    "#list to a df\n",
    "ner_redacted_resumes = pd.DataFrame(ner_corpus_redacted_text)\n",
    "\n",
    "#Save to CSV\n",
    "ner_redacted_resumes.to_csv('ner_redacted_documents_with_id_and_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of redacted documents: {len(ner_corpus_redacted_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(ner_corpus_redacted_text):\n",
    "    print(f\"Redacted Text for Document {i + 1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0404f3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if document_index < len(ner_corpus_redacted_text):\n",
    "    print(f\"Redacted Text for Document 1:\")\n",
    "    print(ner_corpus_redacted_text[document_index])\n",
    "else:\n",
    "    print(\"Document does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4cfee",
   "metadata": {},
   "source": [
    "## spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a143e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = pd.read_csv('Resumes_with_PII_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust this number to scale the size of the corpus\n",
    "resumes = resumes.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f73cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54094559",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ddb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_pipeline(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [{'word': ent.text, 'entity': ent.label_, 'score': ent.kb_id_} for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_documents_spacy_corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fff2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in resumes.iterrows():\n",
    "    document_ID = row['ID']\n",
    "    document_Category = row['Category']\n",
    "    document_text = row['text']\n",
    "\n",
    "    entities = ner_pipeline(document_text)\n",
    "\n",
    "    document_data = {\n",
    "        'ID': document_ID,\n",
    "        'Category': document_Category,\n",
    "        'text': document_text,\n",
    "        'entities': entities  \n",
    "    }\n",
    "\n",
    "    structured_documents_spacy_corpus.append(document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in structured_documents_spacy_corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total documents in structured_documents: {len(structured_documents_spacy_corpus)}\")\n",
    "for doc in structured_documents_spacy_corpus:\n",
    "    print(f\"ID: {doc['ID']}, Entities Count: {len(doc['entities'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_corpus_redacted_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7780427",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'PERSON': 0.75,\n",
    "    'ORG': 0.99,\n",
    "    'GPE': 0.99, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff07fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, document in enumerate(structured_documents_spacy_corpus):\n",
    "    original_text = document['text']\n",
    "    redacted_text = original_text\n",
    "    entities_sorted = sorted(document.get('entities', []), key=lambda x: len(x['word']), reverse=True)\n",
    "\n",
    "    print(f\"Processing Document {i + 1}: {document['ID']}\")\n",
    "    print(f\"Entities from document {i + 1}:\")\n",
    "    \n",
    "    for entity in entities_sorted:\n",
    "        entity_text = entity['word']\n",
    "        entity_label = entity['entity']\n",
    "        score = entity.get('score', 1.0)\n",
    "        \n",
    "       #Makes sure score is treated as a float\n",
    "    score = entity.get('score')\n",
    "    if isinstance(score, str):\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except ValueError:\n",
    "            score = 0.0  #default if conversion fails\n",
    "    else:\n",
    "        score = score if score is not None else 1.0  #fallback to 1.0 if score no score\n",
    " \n",
    "    \n",
    "        if entity_label in thresholds and score >= thresholds[entity_label]:\n",
    "            print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "    for entity in entities_sorted:\n",
    "        entity_text = entity['word']\n",
    "        redacted_text = re.sub(re.escape(entity_text), '[Redacted]', redacted_text)\n",
    "\n",
    "    redacted_text = re.sub(r'\\S+@\\S+', '[Redacted Email]', redacted_text)\n",
    "    redacted_text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[Redacted Phone]', redacted_text)\n",
    "    redacted_text = re.sub(r'\\b(?:https?://)?(?:www\\.)?linkedin\\.com/in/[a-zA-Z0-9._-]+\\b', '[Redacted Website]', redacted_text)\n",
    "    \n",
    "    \n",
    "    spacy_corpus_redacted_text.append({\n",
    "        'ID': document['ID'],  #use document instead of row be wary of rerunning code multiple times, dataset builds upon itsself\n",
    "        'Category': document['Category'],    \n",
    "        'Redacted Text': redacted_text  \n",
    "    \n",
    "    #spacy_corpus_redacted_text.append({\n",
    "       # 'ID': row['ID'],                #row instead of document to build seperate columns in output\n",
    "       # 'Category': row['Category'],    \n",
    "       # 'Redacted Text': redacted_text   \n",
    "    })\n",
    "\n",
    "#list to a df\n",
    "spacy_redacted_resumes = pd.DataFrame(spacy_corpus_redacted_text)\n",
    "\n",
    "#Save to CSV\n",
    "spacy_redacted_resumes.to_csv('spacy_redacted_documents_with_id_and_category.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of redacted documents: {len(spacy_corpus_redacted_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(spacy_corpus_redacted_text):\n",
    "    print(f\"Redacted Text for Document {i + 1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7213c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1429f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if document_index < len(spacy_corpus_redacted_text):\n",
    "    print(f\"Redacted Text for Document 1:\")\n",
    "    print(spacy_corpus_redacted_text[document_index])\n",
    "else:\n",
    "    print(\"Document does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d239d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2cae18a",
   "metadata": {},
   "source": [
    "## observations so far\n",
    "\n",
    "Phone Numbers:\n",
    "- incomplete phone numbers captured, tuples within parenthesis\n",
    "- refine regex pattern\n",
    "\n",
    "[\\+(9[976]\\d|8[987530]\\d|6[987]\\d|5[90]\\d|42\\d|3[875]\\d|\n",
    "2[98654321]\\d|9[8543210]|8[6421]|6[6543210]|5[87654321]|\n",
    "4[987654310]|3[9643210]|2[70]|7|1)\\d{1,14}$] - found on https://stackoverflow.com/questions/2113908/what-regular-expression-will-match-valid-international-phone-numbers\n",
    "- tried to \n",
    "\n",
    "Addresses:\n",
    "- incomplete as well\n",
    "\n",
    "https://stackoverflow.com/questions/55769704/regex-for-matching-a-single-line-standard-usps-address\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d19585",
   "metadata": {},
   "source": [
    "# Miscellenious Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac4e00",
   "metadata": {},
   "source": [
    "## Entity Types in SpaCy: \n",
    "https://towardsdatascience.com/extend-named-entity-recogniser-ner-to-label-new-entities-with-spacy-339ee5979044\n",
    "* = relevent\n",
    "\n",
    "\n",
    "**PERSON - People, including fictional.\n",
    "\n",
    "ORP - Nationalities or religious or political groups.\n",
    "\n",
    "FAC - Buildings, airports, highways, bridges, etc.\n",
    "\n",
    "**ORG - Companies, agencies, institutions, etc.\n",
    "\n",
    "**GPE - Countries, cities, states. *\n",
    "\n",
    "**LOC - Non-GPE locations, mountain ranges, bodies of water. *\n",
    "\n",
    "PRODUCT - Objects, vehicles, foods, etc. (Not services.)\n",
    "\n",
    "EVENT - Named hurricanes, battles, wars, sports events, etc.\n",
    "\n",
    "WORK_OF_ART - Titles of books, songs, etc.\n",
    "\n",
    "LAW - Named documents made into laws.\n",
    "\n",
    "LANGUAGE - Any named language.\n",
    "\n",
    "DATE - Absolute or relative dates or periods.\n",
    "\n",
    "TIME - Times smaller than a day.\n",
    "\n",
    "PERCENT - Percentage, including \"%\".\n",
    "\n",
    "MONEY - Monetary values, including unit.\n",
    "\n",
    "QUANTITY - Measurements, as of weight or distance.\n",
    "\n",
    "ORDINAL - \"first\", \"second\", etc.\n",
    "\n",
    "CARDINAL - Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd793a",
   "metadata": {},
   "source": [
    "# Entity Output Bert-base NER, Distilbert-NER, Bert-large-NER, roberta-large-ner\n",
    "Abbreviation\tDescription\n",
    "O\tOutside of a named entity\n",
    "\n",
    "B-MISC\tBeginning of a miscellaneous entity right after another miscellaneous entity\n",
    "\n",
    "I-MISC\tMiscellaneous entity\n",
    "\n",
    "B-PER\tBeginning of a person’s name right after another person’s name\n",
    "\n",
    "I-PER\tPerson’s name\n",
    "\n",
    "B-ORG\tBeginning of an organization right after another organization\n",
    "\n",
    "I-ORG\torganization\n",
    "\n",
    "B-LOC\tBeginning of a location right after another location\n",
    "\n",
    "I-LOC\tLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bd5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
