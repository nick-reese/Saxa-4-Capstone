{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617dda84",
   "metadata": {},
   "source": [
    "# GENESIS' ORIGINAL CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0a640",
   "metadata": {},
   "source": [
    "# OBJECTIVE: (TO MY BEST UNDERSTANDING)\n",
    "1. Build a Recommender System\n",
    "- Recommender system for matching job descriptions to resumes\n",
    "- recommdner system that suggests jobs to candidates based on resumes\n",
    "- build recommender system for matching job descriptions for gov databases\n",
    "2. Do something with government data\n",
    "3. Build an app\n",
    "\n",
    "\n",
    "Key Methods:\n",
    "- embedding-based recommendations\n",
    "- fine-tuned transformer model\n",
    "\n",
    "## code reactions\n",
    "\n",
    "- embedding models review needed\n",
    "- review langchain embeddings library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5bacdb",
   "metadata": {},
   "source": [
    "# Dez Next Steps\n",
    "\n",
    "1. Reproduce and run Genesis code with redacted pii data\n",
    "2. Add additional LLMs\n",
    "3. Figure Out How to make the prompt engineering run quicker, maybe do 1 resume at a time then slowly increase the dataset size\n",
    "4. Figure Out How difficult is it to get api access to the suggested prompt engineering models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d15f74",
   "metadata": {
    "id": "19d15f74"
   },
   "source": [
    "Scraped Resumes and Prompt Enegineering (GR Version)\n",
    "###LLMs used: GPT4, Cohere and Llama 2\n",
    "###Date: 10/30/2024\n",
    "###Saxa 4#\n",
    "\n",
    "**Data source:**  Used \"modified_file 1.csv” which is the 2,483 resumes that was scraped online. This file was cleaned up into a “more_resume.json” fine. I used this file as a case study for prompt engineering.\n",
    "o\tNote: This code does NOT incorporate PII identification and redaction, which is Dezmond’s code.\n",
    "\n",
    "\n",
    "**Key Highlights of\tPrompt engineering steps:**\n",
    "\n",
    "*Step 1.* --> Embedding: Used Hugging Face \"all-mpnet-base-v2\" as a pre-trained sentence transformer base model designed for generating sentence embeddings (numerical represetnations of sentences that capture semantics (i.e finance, HR, technology etc).\n",
    "\n",
    "*Step 2* -->Prompt Engineering- started shell of code, but did not get code to run as step 1 above took 2+ hours and was still running.\n",
    "\n",
    "*   Prompt Engineering 1: Open AI's GPT 4\n",
    "      -  Updated code to call GR's specific API\n",
    "      - Updated query prompts for resumes, but did not see output yet\n",
    "*   Prompt Engineering 2:  Cohere - need to add in Specific API\n",
    "*   Prompt Engineering 3:  Llama 2- Requested user access request to Meta via Hugging Face, awaiting access approval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f6574",
   "metadata": {
    "id": "be9f6574"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VIOtS5IW8r0w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIOtS5IW8r0w",
    "outputId": "63e1f7be-0d6d-43d6-80c6-fc1a7948d08c"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install python-docx==1.1.0\n",
    "!pip install pdfplumber==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e171d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d46e171d",
    "outputId": "46ece2c0-b40b-42cd-f25f-c4e929ea9e02"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from docx import Document\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pdfplumber\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wUfmuuT4_-Wv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUfmuuT4_-Wv",
    "outputId": "b55ba029-69d7-4091-f0a3-3d6766cb9c34"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ca3b6",
   "metadata": {
    "id": "c65ca3b6"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fec546",
   "metadata": {
    "id": "39fec546"
   },
   "source": [
    "## Adding in the Other Resumes for Prompt Engineering exercise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e186bfd",
   "metadata": {
    "id": "0e186bfd"
   },
   "outputs": [],
   "source": [
    "more_resumes = pd.read_csv('/content/drive/My Drive/Capstone(Saxa 4)/04Code/modified_file 1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4f1fe",
   "metadata": {
    "id": "3ff4f1fe"
   },
   "outputs": [],
   "source": [
    "more_resumes.to_json('more_resumes.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be42bd",
   "metadata": {
    "id": "71be42bd"
   },
   "outputs": [],
   "source": [
    "more_resumes.rename(columns = {'Resume_str': 'text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052be8fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "052be8fe",
    "outputId": "e43b1308-ac96-446e-8c76-13045fdff0c8"
   },
   "outputs": [],
   "source": [
    "more_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e6f60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "334e6f60",
    "outputId": "ab8bfa39-5afa-4485-833a-344d8565ce3b"
   },
   "outputs": [],
   "source": [
    "more_resumes = pd.DataFrame(more_resumes)\n",
    "more_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0693e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "da0693e4",
    "outputId": "8b7fcc73-9c9f-4160-ecad-6d90a56d92d4"
   },
   "outputs": [],
   "source": [
    "more_resumes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83650ce1",
   "metadata": {
    "id": "83650ce1"
   },
   "outputs": [],
   "source": [
    "#more_resumes['text'] = more_resumes['text'].apply(lambda x: x.encode('utf-8').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b0013",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "a89b0013",
    "outputId": "af139e84-96ab-4692-d84b-f0923824ddbf"
   },
   "outputs": [],
   "source": [
    "more_resumes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1e25b",
   "metadata": {
    "id": "02f1e25b"
   },
   "outputs": [],
   "source": [
    "#more_resumes.drop(columns = ['Resume_html'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da510d6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "da510d6f",
    "outputId": "71fc29cc-f01b-4c31-ede7-e880d259e4cf"
   },
   "outputs": [],
   "source": [
    "more_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ddb088",
   "metadata": {
    "id": "16ddb088"
   },
   "outputs": [],
   "source": [
    "more_resumes.columns = more_resumes.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c23ac",
   "metadata": {
    "id": "875c23ac"
   },
   "outputs": [],
   "source": [
    "more_resumes['text'] = more_resumes['text'].apply(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76e1d9",
   "metadata": {
    "id": "ac76e1d9"
   },
   "outputs": [],
   "source": [
    "more_resumes['text'] = more_resumes['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995b254",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "2995b254",
    "outputId": "d0c1f699-b34c-4fa3-8489-b3c217d595d2"
   },
   "outputs": [],
   "source": [
    "more_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c08d99",
   "metadata": {
    "id": "16c08d99"
   },
   "outputs": [],
   "source": [
    "more_resumes['text'] = more_resumes['text'].str.replace('[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4a00a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9b4a00a",
    "outputId": "9263db9a-0e9b-439f-d431-aaeb4602241e"
   },
   "outputs": [],
   "source": [
    "cats_more_res = more_resumes['Category'].nunique()\n",
    "cats_more_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86713c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "af86713c",
    "outputId": "cf24b557-fa97-49d3-be3e-a4531361eed5"
   },
   "outputs": [],
   "source": [
    "cat_counts = more_resumes['Category'].value_counts()    #Look at CSV and get get rid of the 1's\n",
    "cat_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29960eee",
   "metadata": {
    "id": "29960eee"
   },
   "outputs": [],
   "source": [
    "finance = more_resumes[more_resumes['Category'] == 'FINANCE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd62768",
   "metadata": {
    "id": "cdd62768"
   },
   "outputs": [],
   "source": [
    "engineering = more_resumes[more_resumes['Category'] == 'ENGINEERING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rGjcui4TsPrv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "rGjcui4TsPrv",
    "outputId": "1140e69e-fba0-4d6d-e453-a48300727c3f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "more_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_gbzNSuNsjjf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_gbzNSuNsjjf",
    "outputId": "f9a40baf-f5bc-467f-e9a3-d63babdc723f"
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.240 transformers sentence_transformers datasets faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tvQ_8HlK4tLp",
   "metadata": {
    "id": "tvQ_8HlK4tLp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bcv-ebB4wSj",
   "metadata": {
    "id": "0bcv-ebB4wSj"
   },
   "source": [
    "### PROMPT ENGINEERING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7hFNhMYsnAu",
   "metadata": {
    "id": "b7hFNhMYsnAu"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document # Import the Document class\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b6cfd",
   "metadata": {},
   "source": [
    "# Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O5GwI8u1shZ5",
   "metadata": {
    "id": "O5GwI8u1shZ5"
   },
   "outputs": [],
   "source": [
    "# 1. Load the Hugging Face Embeddings model\n",
    "  ##Notes##\n",
    "   # Code Purpose: loads a pre-trained sentence transformer model called \"all-mpnet-base-v2\" from Hugging Face.\n",
    "   # Model insight: \"all-mpnet-base-v2\" is used to convert text (in this case, resume content) into numerical vectors called embeddings.\n",
    "   # Functionality: Embeddings capture the semantic meaning of the text, allowing you to compare the similarity between different pieces of text based on their vector representations.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# 2. Load the more_resumes dataset\n",
    "  # Assumption about dataset: Assuming 'more_resumes.json' is in the current directory\n",
    "more_resumes = load_dataset('json', data_files='more_resumes.json')\n",
    "\n",
    "# 3. Split resumes into smaller chunks for embedding\n",
    "  #Code purpose: Divides each resume's text content into smaller chunks to handle potentially large documents.\n",
    "  # Functionality: RecursiveCharacterTextSplitter: This object is used to split the text. It's configured to create\n",
    "  # chunks of approximately 1000 characters with an overlap of 200 characters between chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = []\n",
    "for resume in more_resumes['train']:\n",
    "    texts = text_splitter.split_text(resume['text'])\n",
    "    # Create Document objects instead of dictionaries\n",
    "    docs.extend([Document(page_content=t, metadata={'source': resume['Category']}) for t in texts])\n",
    "\n",
    "# 4. Embed the resume chunks and store them in a FAISS vectorstore\n",
    "  #Purpose: Purpose: Creates a vector database using FAISS (Facebook AI Similarity Search) and\n",
    "  # stores the embeddings of the resume chunks in it.\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "db.save_local(\"more_resumes_faiss\")\n",
    "\n",
    "# Now the 'more_resumes' dataset is embedded and stored in a FAISS vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1CB-iTi40QO7",
   "metadata": {
    "id": "1CB-iTi40QO7"
   },
   "outputs": [],
   "source": [
    "###Prompt Engineering 1: Open AI's GPT 4 ###\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import os        #for API Key\n",
    "import openai\n",
    "\n",
    "\n",
    "# 1. Load the FAISS vectorstore\n",
    "db = FAISS.load_local(\"more_resumes_faiss\", embeddings)\n",
    "\n",
    "# 2. Initialize OpenAI's GPT-4\n",
    "# Make sure you set your OPENAI_API_KEY environment variable\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Set's GR's API key as an environment variable (recommended)\n",
    "os.environ[\"genesis_gpt4_API_key\"] = \"genesis_gpt4_API_key\" #Replace YOUR_API_KEY with your actual OpenAI key\n",
    "\n",
    "# 3. Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# 4. Define your prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't have enough information to answer the question, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Function to run a query\n",
    "def run_query(query):\n",
    "    \"\"\"Runs a query against the resume dataset using prompt engineering.\"\"\"\n",
    "    response = qa_chain.run(\n",
    "        prompt_template.format(context=db.as_retriever().get_relevant_documents(query)[0].page_content, question=query)\n",
    "    )\n",
    "    return response #Corrected to return the response\n",
    "\n",
    "# 6a. Example usage 1\n",
    "query = \"Based on this resume, what are the top jobs I can qualify for?\"\n",
    "result = run_query(query)\n",
    "print(result)\n",
    "\n",
    "# 6b. Example usage 2\n",
    "query1 = \"I live in a high cost city and need to make at least $80k salary per year, based on my resume, what jobs should I apply for to get me this salary?\"\n",
    "result1 = run_query(query1)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nIxUKpZHw-61",
   "metadata": {
    "id": "nIxUKpZHw-61"
   },
   "outputs": [],
   "source": [
    "###Prompt Engineering 2:  Cohere ###\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Cohere\n",
    "\n",
    "# 1. Load the FAISS vectorstore\n",
    "db = FAISS.load_local(\"more_resumes_faiss\", embeddings)\n",
    "\n",
    "# 2. Initialize Cohere\n",
    "# Make sure you set your COHERE_API_KEY environment variable\n",
    "llm = Cohere(model=\"command-xlarge-nightly\", temperature=0)\n",
    "\n",
    "# 3. Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# 4. Define your prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't have enough information to answer the question, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Function to run a query\n",
    "def run_query(query):\n",
    "    \"\"\"Runs a query against the resume dataset using prompt engineering.\"\"\"\n",
    "    response = qa_chain.run(\n",
    "        prompt_template.format(context=db.as_retriever().get_relevant_documents(query)[0].page_content, question=query)\n",
    "    )\n",
    "    return response #Corrected to return the response\n",
    "\n",
    "# 6. Example usage\n",
    "query = \"What are the skills of candidates in the Finance category?\"\n",
    "result = run_query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tBK_Mh4N1GoU",
   "metadata": {
    "id": "tBK_Mh4N1GoU"
   },
   "outputs": [],
   "source": [
    "###Prompt Engineering 3:  Llama 2 ###\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "# 1. Load the FAISS vectorstore\n",
    "db = FAISS.load_local(\"more_resumes_faiss\", embeddings)\n",
    "\n",
    "# 2. Initialize Llama 2\n",
    "# Replace with the path to your Llama 2 model file\n",
    "llm = LlamaCpp(model_path=\"/path/to/your/llama-2-7b-chat.ggmlv3.q4_0.bin\")\n",
    "\n",
    "\n",
    "# 3. Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# 4. Define your prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't have enough information to answer the question, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Function to run a query\n",
    "def run_query(query):\n",
    "    \"\"\"Runs a query against the resume dataset using prompt engineering.\"\"\"\n",
    "    response = qa_chain.run(\n",
    "        prompt_template.format(context=db.as_retriever().get_relevant_documents(query)[0].page_content, question=query)\n",
    "    )\n",
    "    return response #Corrected to return the response\n",
    "\n",
    "# 6. Example usage\n",
    "query = \"What are the skills of candidates in the Finance category?\"\n",
    "result = run_query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d065f4",
   "metadata": {
    "id": "RPnv3Y28sl6f"
   },
   "source": [
    "# Testing Code with Redacted PII data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1D7OaRBssiW5",
   "metadata": {
    "id": "1D7OaRBssiW5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dezri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "#from docx import Document\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pdfplumber\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import os\n",
    "import json\n",
    "#from docx import Document\n",
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from plotly.offline import plot\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d931146",
   "metadata": {},
   "source": [
    "# Redacted with spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b40c3",
   "metadata": {},
   "source": [
    "# adding Genesis Code\n",
    "- dataset csv name = spacy_redacted_documents_with_id_and_category.csv\n",
    "https://python.langchain.com/docs/introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876964d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34387d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dad24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain-huggingface tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d72411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.11.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90968c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dezri\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: keras in c:\\users\\dezri\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Collecting keras\n",
      "  Obtaining dependency information for keras from https://files.pythonhosted.org/packages/c2/88/eef50051a772dcb4433d1f3e4c1d6576ba450fe83e89d028d7e8b85a2122/keras-3.6.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: rich in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from keras) (13.8.0)\n",
      "Requirement already satisfied: namex in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.1)\n",
      "Using cached keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.11.0\n",
      "    Uninstalling keras-2.11.0:\n",
      "      Successfully uninstalled keras-2.11.0\n",
      "Successfully installed keras-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c225770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Obtaining dependency information for faiss-cpu from https://files.pythonhosted.org/packages/87/2b/850200d901fd0409232d9bcae26228ab9aadf4803bbcc38f93d6f81cab37/faiss_cpu-1.9.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading faiss_cpu-1.9.0-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.9.0-cp311-cp311-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB 640.0 kB/s eta 0:00:24\n",
      "   ---------------------------------------- 0.1/14.9 MB 787.7 kB/s eta 0:00:19\n",
      "    --------------------------------------- 0.2/14.9 MB 1.8 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.5/14.9 MB 3.5 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.7/14.9 MB 3.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.2/14.9 MB 4.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.6/14.9 MB 5.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.9/14.9 MB 6.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.3/14.9 MB 6.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.6/14.9 MB 7.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.9/14.9 MB 7.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.2/14.9 MB 7.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.4/14.9 MB 7.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.9/14.9 MB 7.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.4/14.9 MB 7.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.8/14.9 MB 7.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.4/14.9 MB 8.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.4/14.9 MB 8.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.4/14.9 MB 8.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.4/14.9 MB 8.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.6/14.9 MB 7.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.9/14.9 MB 7.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.1/14.9 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.8/14.9 MB 7.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.2/14.9 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.5/14.9 MB 7.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.2/14.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 8.5/14.9 MB 8.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.9/14.9 MB 8.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.4/14.9 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.8/14.9 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.2/14.9 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.5/14.9 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.0/14.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.1/14.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.7/14.9 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.2/14.9 MB 9.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.6/14.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.0/14.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.3/14.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.6/14.9 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.0/14.9 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.4/14.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.9/14.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400a7210",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from datasets import load_dataset\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c3edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c1228",
   "metadata": {},
   "source": [
    "# SENTENCE TRANSFORMER MODELS\n",
    "For a recommender system that **matches job descriptions to resumes**, you’ll want an LLM with strong natural language understanding and embeddings capabilities\n",
    "\n",
    "cost-effective, open-source solution - \n",
    "SBERT would allow you to keep everything in-house and potentially run it more efficiently.\n",
    "\n",
    "- https://www.sbert.net/index.html\n",
    "\n",
    "The all-* models were trained on all available training data (more than 1 billion training pairs) and are designed as general purpose models. \n",
    "\n",
    "Pretrained models\n",
    " - https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    " \n",
    " all-mpnet-base-v2 - https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "   - Model insight: \"all-mpnet-base-v2\" is used to convert text (in this case, resume content) into numerical vectors called embeddings.\n",
    "   - Functionality: Embeddings capture the semantic meaning of the text, allowing you to compare the similarity between different pieces of text based on their vector representations.\n",
    " \n",
    "all-distilroberta-v1 - https://huggingface.co/sentence-transformers/all-distilroberta-v1\n",
    "\n",
    "all-MiniLM-L12-v2 - https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c531452",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the Hugging Face Embeddings model\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# 2. Load the more_resumes dataset\n",
    "\n",
    "redacted_resumes = pd.read_csv('spacy_redacted_documents_with_id_and_category.csv')\n",
    "redacted_resumes = redacted_resumes.head(5) #use to limit dataset for testing\n",
    "\n",
    "# 3. Split resumes into smaller chunks for embedding\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = []\n",
    "for index, resume in more_resumes.iterrows(): \n",
    "    texts = text_splitter.split_text(resume['Redacted Text'])  \n",
    "    docs.extend([Document(page_content=t, metadata={'source': resume['Category']}) for t in texts]) \n",
    "# 4. Embed the resume chunks and store them in a FAISS vectorstore\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "db.save_local(\"redacted_resumes_faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0fd3d9",
   "metadata": {},
   "source": [
    "# Requires Login and API Keys - Free Trails May be Available\n",
    "\n",
    "Groq API Instructions - https://docs.aicontentlabs.com/articles/groq-api-key/\n",
    "Cohere Instructions - https://docs.aicontentlabs.com/articles/cohere-api-key/\n",
    "Open AI Instructions - https://docs.aicontentlabs.com/articles/openai-api-key/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc657809",
   "metadata": {
    "id": "1CB-iTi40QO7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dezri\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\openai.py:255: UserWarning:\n",
      "\n",
      "You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIChat\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {'model_...allowed_special': 'all'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mload_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredacted_resumes_faiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings, allow_dangerous_deserialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 2. Initialize OpenAI's GPT-4\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Make sure you set your OPENAI_API_KEY environment variable\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Set's GR's API key as an environment variable (recommended)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenesis_gpt4_API_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenesis_gpt4_API_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#Replace YOUR_API_KEY with your actual OpenAI key\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\openai.py:260\u001b[0m, in \u001b[0;36mBaseOpenAI.__new__\u001b[1;34m(cls, **data)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    253\u001b[0m     model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[0;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to use a chat model. This way of initializing it is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno longer supported. Instead, please use: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from langchain_community.chat_models import ChatOpenAI`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m     )\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenAIChat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for OpenAIChat\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {'model_...allowed_special': 'all'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
     ]
    }
   ],
   "source": [
    "###Prompt Engineering 1: Open AI's GPT 4 ###\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import os        #for API Key\n",
    "import openai\n",
    "\n",
    "\n",
    "# 1. Load the FAISS vectorstore\n",
    "db = FAISS.load_local(\"redacted_resumes_faiss\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 2. Initialize OpenAI's GPT-4\n",
    "# Make sure you set your OPENAI_API_KEY environment variable\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Set's GR's API key as an environment variable (recommended)\n",
    "os.environ[\"genesis_gpt4_API_key\"] = \"genesis_gpt4_API_key\" #Replace YOUR_API_KEY with your actual OpenAI key\n",
    "\n",
    "# 3. Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# 4. Define your prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't have enough information to answer the question, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Function to run a query\n",
    "def run_query(query):\n",
    "    \"\"\"Runs a query against the resume dataset using prompt engineering.\"\"\"\n",
    "    response = qa_chain.run(\n",
    "        prompt_template.format(context=db.as_retriever().get_relevant_documents(query)[0].page_content, question=query)\n",
    "    )\n",
    "    return response #Corrected to return the response\n",
    "\n",
    "# 6a. Example usage 1\n",
    "query = \"Based on this resume, what are the top jobs I can qualify for?\"\n",
    "result = run_query(query)\n",
    "print(result)\n",
    "\n",
    "# 6b. Example usage 2\n",
    "query1 = \"I live in a high cost city and need to make at least $80k salary per year, based on my resume, what jobs should I apply for to get me this salary?\"\n",
    "result1 = run_query(query1)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b64d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ccfda6a",
   "metadata": {
    "id": "nIxUKpZHw-61"
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Cohere\n  Value error, Did not find cohere_api_key, please add an environment variable `COHERE_API_KEY` which contains it, or pass `cohere_api_key` as a named parameter. [type=value_error, input_value={'model': 'command-xlarge...None, 'max_retries': 10}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mload_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredacted_resumes_faiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings, allow_dangerous_deserialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Initialize Cohere\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Make sure you set your COHERE_API_KEY environment variable\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m llm \u001b[38;5;241m=\u001b[39m Cohere(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommand-xlarge-nightly\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 3. Create a RetrievalQA chain\u001b[39;00m\n\u001b[0;32m     14\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[0;32m     15\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m     16\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mdb\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Cohere\n  Value error, Did not find cohere_api_key, please add an environment variable `COHERE_API_KEY` which contains it, or pass `cohere_api_key` as a named parameter. [type=value_error, input_value={'model': 'command-xlarge...None, 'max_retries': 10}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
     ]
    }
   ],
   "source": [
    "###Prompt Engineering 2:  Cohere ###\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Cohere\n",
    "\n",
    "# 1. Load the FAISS vectorstore\n",
    "db = FAISS.load_local(\"redacted_resumes_faiss\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 2. Initialize Cohere\n",
    "# Make sure you set your COHERE_API_KEY environment variable\n",
    "llm = Cohere(model=\"command-xlarge-nightly\", temperature=0)\n",
    "\n",
    "# 3. Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# 4. Define your prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't have enough information to answer the question, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Function to run a query\n",
    "def run_query(query):\n",
    "    \"\"\"Runs a query against the resume dataset using prompt engineering.\"\"\"\n",
    "    response = qa_chain.run(\n",
    "        prompt_template.format(context=db.as_retriever().get_relevant_documents(query)[0].page_content, question=query)\n",
    "    )\n",
    "    return response #Corrected to return the response\n",
    "\n",
    "# 6. Example usage\n",
    "query = \"What are the skills of candidates in the HR category?\"\n",
    "result = run_query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7164592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n",
      "     ---------------------------------------- 0.0/63.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/63.9 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/63.9 MB 435.7 kB/s eta 0:02:27\n",
      "     --------------------------------------- 0.1/63.9 MB 525.1 kB/s eta 0:02:02\n",
      "     ---------------------------------------- 0.2/63.9 MB 1.3 MB/s eta 0:00:49\n",
      "     ---------------------------------------- 0.3/63.9 MB 2.0 MB/s eta 0:00:32\n",
      "     ---------------------------------------- 0.7/63.9 MB 3.7 MB/s eta 0:00:18\n",
      "      --------------------------------------- 1.0/63.9 MB 4.4 MB/s eta 0:00:15\n",
      "      --------------------------------------- 1.4/63.9 MB 5.1 MB/s eta 0:00:13\n",
      "     - -------------------------------------- 1.7/63.9 MB 5.6 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 2.2/63.9 MB 6.3 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 2.6/63.9 MB 6.7 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 3.0/63.9 MB 7.1 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 3.4/63.9 MB 7.4 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 4.1/63.9 MB 8.1 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 4.5/63.9 MB 8.5 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 5.1/63.9 MB 8.8 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 5.5/63.9 MB 9.2 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 5.7/63.9 MB 9.4 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 6.4/63.9 MB 9.3 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 6.4/63.9 MB 9.3 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 6.5/63.9 MB 8.6 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 7.0/63.9 MB 8.8 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 7.4/63.9 MB 9.0 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 7.6/63.9 MB 8.7 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 8.4/63.9 MB 9.3 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 9.0/63.9 MB 9.5 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 9.6/63.9 MB 9.7 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 10.0/63.9 MB 9.7 MB/s eta 0:00:06\n",
      "     ------ -------------------------------- 10.4/63.9 MB 10.9 MB/s eta 0:00:05\n",
      "     ------ -------------------------------- 10.4/63.9 MB 10.9 MB/s eta 0:00:05\n",
      "     ------ -------------------------------- 11.4/63.9 MB 11.7 MB/s eta 0:00:05\n",
      "     ------- ------------------------------- 11.7/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     ------- ------------------------------- 12.0/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     ------- ------------------------------- 12.5/63.9 MB 11.7 MB/s eta 0:00:05\n",
      "     ------- ------------------------------- 12.9/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------ 13.4/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------ 14.1/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------ 14.3/63.9 MB 11.3 MB/s eta 0:00:05\n",
      "     -------- ------------------------------ 14.6/63.9 MB 11.3 MB/s eta 0:00:05\n",
      "     --------- ----------------------------- 15.2/63.9 MB 11.3 MB/s eta 0:00:05\n",
      "     --------- ----------------------------- 15.4/63.9 MB 11.1 MB/s eta 0:00:05\n",
      "     --------- ----------------------------- 16.0/63.9 MB 11.3 MB/s eta 0:00:05\n",
      "     ---------- ---------------------------- 16.5/63.9 MB 11.3 MB/s eta 0:00:05\n",
      "     ---------- ---------------------------- 16.8/63.9 MB 11.9 MB/s eta 0:00:04\n",
      "     ---------- ---------------------------- 17.1/63.9 MB 11.5 MB/s eta 0:00:05\n",
      "     ---------- ---------------------------- 17.7/63.9 MB 11.9 MB/s eta 0:00:04\n",
      "     ----------- --------------------------- 18.4/63.9 MB 12.3 MB/s eta 0:00:04\n",
      "     ----------- --------------------------- 18.8/63.9 MB 11.9 MB/s eta 0:00:04\n",
      "     ----------- --------------------------- 19.1/63.9 MB 11.9 MB/s eta 0:00:04\n",
      "     ----------- --------------------------- 19.1/63.9 MB 11.9 MB/s eta 0:00:04\n",
      "     ------------ -------------------------- 19.9/63.9 MB 11.5 MB/s eta 0:00:04\n",
      "     ------------ -------------------------- 20.2/63.9 MB 11.5 MB/s eta 0:00:04\n",
      "     ------------ -------------------------- 20.6/63.9 MB 10.9 MB/s eta 0:00:04\n",
      "     ------------ -------------------------- 21.0/63.9 MB 11.7 MB/s eta 0:00:04\n",
      "     ------------ -------------------------- 21.2/63.9 MB 11.3 MB/s eta 0:00:04\n",
      "     ------------- ------------------------- 22.1/63.9 MB 11.5 MB/s eta 0:00:04\n",
      "     ------------- ------------------------- 22.1/63.9 MB 11.1 MB/s eta 0:00:04\n",
      "     ------------- ------------------------- 22.7/63.9 MB 10.9 MB/s eta 0:00:04\n",
      "     -------------- ------------------------ 22.9/63.9 MB 10.9 MB/s eta 0:00:04\n",
      "     -------------- ------------------------ 23.1/63.9 MB 11.3 MB/s eta 0:00:04\n",
      "     -------------- ------------------------ 23.1/63.9 MB 11.3 MB/s eta 0:00:04\n",
      "     -------------- ------------------------ 23.5/63.9 MB 10.2 MB/s eta 0:00:04\n",
      "     -------------- ------------------------ 23.6/63.9 MB 10.1 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 23.8/63.9 MB 9.9 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 24.2/63.9 MB 9.6 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 24.5/63.9 MB 9.8 MB/s eta 0:00:05\n",
      "     --------------- ----------------------- 24.9/63.9 MB 10.1 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 25.2/63.9 MB 9.8 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 25.5/63.9 MB 9.5 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 25.9/63.9 MB 9.5 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 26.0/63.9 MB 9.4 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 26.5/63.9 MB 9.2 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 26.7/63.9 MB 9.1 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 27.1/63.9 MB 9.4 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 27.5/63.9 MB 9.1 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 27.8/63.9 MB 8.7 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 28.2/63.9 MB 8.8 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 28.4/63.9 MB 8.8 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 28.8/63.9 MB 8.7 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 28.9/63.9 MB 8.3 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 29.3/63.9 MB 8.4 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 29.8/63.9 MB 8.6 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 30.2/63.9 MB 8.5 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 30.4/63.9 MB 8.3 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 30.6/63.9 MB 8.3 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 31.0/63.9 MB 8.3 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 31.0/63.9 MB 8.2 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 31.4/63.9 MB 8.3 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 31.9/63.9 MB 8.1 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 32.2/63.9 MB 7.9 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 32.6/63.9 MB 8.1 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 33.1/63.9 MB 8.1 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 33.3/63.9 MB 8.1 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 33.6/63.9 MB 8.5 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 34.0/63.9 MB 8.7 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 34.4/63.9 MB 8.7 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 34.7/63.9 MB 8.6 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 34.9/63.9 MB 8.7 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 35.2/63.9 MB 8.6 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 35.5/63.9 MB 8.7 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 35.5/63.9 MB 8.7 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 35.9/63.9 MB 8.5 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 36.0/63.9 MB 8.4 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 36.0/63.9 MB 8.4 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 36.1/63.9 MB 7.9 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 36.6/63.9 MB 7.9 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 36.6/63.9 MB 7.9 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 37.0/63.9 MB 7.9 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 37.2/63.9 MB 7.8 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 37.6/63.9 MB 7.7 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 38.0/63.9 MB 7.9 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 38.4/63.9 MB 7.8 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 38.6/63.9 MB 7.7 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 39.1/63.9 MB 8.0 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 39.3/63.9 MB 8.0 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 39.7/63.9 MB 8.0 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 39.7/63.9 MB 7.7 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 40.2/63.9 MB 7.8 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 40.6/63.9 MB 7.8 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 41.1/63.9 MB 7.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 41.1/63.9 MB 7.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 41.7/63.9 MB 8.2 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 42.1/63.9 MB 8.1 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 42.6/63.9 MB 8.1 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 42.7/63.9 MB 8.1 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 43.0/63.9 MB 8.0 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 43.1/63.9 MB 7.8 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 43.7/63.9 MB 7.8 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 44.0/63.9 MB 8.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 44.2/63.9 MB 7.8 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 44.7/63.9 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 45.1/63.9 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 45.4/63.9 MB 8.0 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 45.5/63.9 MB 8.0 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 45.5/63.9 MB 8.0 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 45.9/63.9 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 46.2/63.9 MB 8.3 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 46.6/63.9 MB 8.3 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 47.1/63.9 MB 8.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 47.7/63.9 MB 8.7 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 48.1/63.9 MB 8.7 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 48.5/63.9 MB 8.8 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 48.8/63.9 MB 8.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 49.2/63.9 MB 9.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 49.2/63.9 MB 9.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 49.7/63.9 MB 8.6 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 50.0/63.9 MB 9.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 50.5/63.9 MB 9.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 51.0/63.9 MB 9.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 51.0/63.9 MB 9.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 51.6/63.9 MB 8.8 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 52.0/63.9 MB 9.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 52.3/63.9 MB 9.0 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 52.8/63.9 MB 9.0 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 53.1/63.9 MB 9.2 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 53.5/63.9 MB 9.4 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 54.0/63.9 MB 9.2 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 54.1/63.9 MB 9.4 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 54.8/63.9 MB 9.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 54.9/63.9 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 55.4/63.9 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 55.8/63.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 55.9/63.9 MB 9.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 56.3/63.9 MB 9.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 56.5/63.9 MB 9.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 56.9/63.9 MB 9.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 57.3/63.9 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 57.8/63.9 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 58.3/63.9 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 58.7/63.9 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 59.1/63.9 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 59.2/63.9 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 59.6/63.9 MB 9.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 60.0/63.9 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 60.0/63.9 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 60.0/63.9 MB 9.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 60.9/63.9 MB 9.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 61.0/63.9 MB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 61.4/63.9 MB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 61.6/63.9 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 61.9/63.9 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  62.4/63.9 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  62.6/63.9 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.0/63.9 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.2/63.9 MB 8.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.5/63.9 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.6/63.9 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.6/63.9 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  63.9/63.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 63.9/63.9 MB 6.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from llama-cpp-python) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dezri\\anaconda3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.1)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.5/45.5 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [20 lines of output]\n",
      "  \u001b[32m*** \u001b[1mscikit-build-core 0.10.7\u001b[0m using \u001b[34mCMake 3.30.5\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "  \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  2024-11-01 07:09:55,603 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "  loading initial cache file C:\\Users\\dezri\\AppData\\Local\\Temp\\tmp5e7c_rdr\\build\\CMakeInit.txt\n",
      "  -- Building for: NMake Makefiles\n",
      "  CMake Error at CMakeLists.txt:3 (project):\n",
      "    Running\n",
      "  \n",
      "     'nmake' '-?'\n",
      "  \n",
      "    failed with:\n",
      "  \n",
      "     no such file or directory\n",
      "  \n",
      "  \n",
      "  CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "  CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  \u001b[31m\n",
      "  \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b23565d4",
   "metadata": {
    "id": "tBK_Mh4N1GoU"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\llamacpp.py:140\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mload_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredacted_resumes_faiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings, allow_dangerous_deserialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Initialize Llama 2\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Replace with the path to your Llama 2 model file\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m llm \u001b[38;5;241m=\u001b[39m LlamaCpp(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/your/llama-2-7b-chat.ggmlv3.q4_0.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 3. Create a RetrievalQA chain\u001b[39;00m\n\u001b[0;32m     15\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[0;32m     16\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m     17\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mdb\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[0;32m     19\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_decorators_v1.py:148\u001b[0m, in \u001b[0;36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[1;34m(values, _)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema\u001b[38;5;241m.\u001b[39mValidationInfo) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RootValidatorValues:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validator(values)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\utils\\pydantic.py:208\u001b[0m, in \u001b[0;36mpre_init.<locals>.wrapper\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    205\u001b[0m             values[name] \u001b[38;5;241m=\u001b[39m field_info\u001b[38;5;241m.\u001b[39mdefault\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mcls\u001b[39m, values)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\llamacpp.py:142\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import llama-cpp-python library. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the llama-cpp-python library to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    148\u001b[0m model_path \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    149\u001b[0m model_param_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    166\u001b[0m ]\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "###Prompt Engineering 3:  Llama 2 ###\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "# 1. Load the FAISS vectorstore\n",
    "db = FAISS.load_local(\"redacted_resumes_faiss\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 2. Initialize Llama 2\n",
    "# Replace with the path to your Llama 2 model file\n",
    "llm = LlamaCpp(model_path=\"/path/to/your/llama-2-7b-chat.ggmlv3.q4_0.bin\")\n",
    "\n",
    "\n",
    "# 3. Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# 4. Define your prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't have enough information to answer the question, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Function to run a query\n",
    "def run_query(query):\n",
    "    \"\"\"Runs a query against the resume dataset using prompt engineering.\"\"\"\n",
    "    response = qa_chain.run(\n",
    "        prompt_template.format(context=db.as_retriever().get_relevant_documents(query)[0].page_content, question=query)\n",
    "    )\n",
    "    return response #Corrected to return the response\n",
    "\n",
    "# 6. Example usage\n",
    "query = \"What are the skills of candidates in the Finance category?\"\n",
    "result = run_query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb65b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
